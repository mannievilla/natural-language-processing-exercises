{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af7d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0f27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acquire import get_blog_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b016816",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_blog_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e32c3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'May is traditionally known as Asian American and Pacific Islander (AAPI) Heritage Month. This month we celebrate the history and contributions made possible by our AAPI friends, family, and community. We also examine our level of support and seek opportunities to better understand the AAPI community.\\n\\nIn an effort to address real concerns and experiences, we sat down with Arbeena Thapa, one of Codeup’s Financial Aid and Enrollment Managers.\\nArbeena identifies as Nepali American and Desi. Arbeena’s parents immigrated to Texas in 1988 for better employment and educational opportunities. Arbeena’s older sister was five when they made the move to the US. Arbeena was born later, becoming the first in her family to be a US citizen.\\nAt Codeup we take our efforts at inclusivity very seriously. After speaking with Arbeena, we were taught that the term AAPI excludes Desi-American individuals. Hence, we will now use the term Asian Pacific Islander Desi American (APIDA).\\nHere is how the rest of our conversation with Arbeena went!\\nHow do you celebrate or connect with your heritage and cultural traditions?\\n“I celebrate Nepal’s version of Christmas or Dashain. This is a nine-day celebration also known as Dussehra. I grew up as Hindu and I identify as Hindu, this is a very large part of my heritage. “\\n“Other ways I connect with my culture include sharing food! Momos are South Asian Dumplings and they’re my favorite to make and share.”\\n“On my Asian American side, I am an advocate of immigrant justice and erasure within APIDA social or political movements. I participate in events to embrace my identity such as immigrant justice advocacy because I come from a mixed-status family. I’ve always been in a community with undocumented Asian immigrants. .”\\nWhat are some of the challenges you have faced as an APIDA individual, personally or professionally?\\n“I often struggle with being gendered as compliant or a pushover. Professionally, I am often stereotyped as meek, so I’ve been overlooked for leadership roles. We are seen as perpetually foreign; people tend to other us in that way, yet put us on a pedestal for what a model minority looks like. This has made me hesitant to share my heritage in the past because these assumptions get mapped onto me. ”\\nCan you describe some common barriers of entry that APIDA individuals, specifically women may face when trying to enter or advance in the workplace?\\n“Being overlooked for leadership. In the past, I have not been viewed as a leader. People sometimes have preconceived stereotypes of Asian women not being able to be bold, or being vocal can be mistaken for being too emotional. “\\nHow do you believe microaggressions impact APIDA individuals in the workplace? Can you provide examples of such microaggressions?\\n“Erasure is big. To me, only saying ‘Merry Christmas’ isn’t inclusive to other religions. People are often resistant to saying ‘Happy Holidays,’ but saying Merry Christmas excludes, and does not appreciate my heritage. “\\n“Often microaggressions are not micro at all. They typically are not aggressive racialized violence, but the term ‘micro’ minimizes impact.”\\n“Some that I’ve heard are ‘What kind of Asian are you?’ or ‘Where are you from?’ This automatically makes me the ‘other’ and not seen as American. Even within the APIDA community, South Asians are overlooked as “Asian”.”\\nHow important is representation, specifically APIDA representation, in organizational leadership positions?\\n“I want to say that it is important to have someone who looks like you in leadership roles, and it is, but those leaders may not share the same beliefs as you. Certain privileges such as wealth, resources, or lack of interaction with lower-socioeconomic-status Asian Americans may cause a difference in community politics. I do not think the bamboo ceiling is acceptable, but the company you work for plays a big part in your politics and belief alignment.”\\nHow do you feel about code-switching, and have you ever felt it necessary to code-switch?\\n“I like sharing South Asian terms or connecting with others that have similar heritage and culture. A workplace that is welcoming to going into this sort of breakout is refreshing and makes space for us. However, having to code-switch could also mean a workplace that is not conducive and welcoming of other cultures. “\\nFinally, in your opinion, what long-term strategies can create lasting change in the workplace and ensure support, equality, and inclusion for APIDA individuals?\\n“Prior to a career in financial aid, I did a lot of research related to the post-9/11 immigration of the South Asian diaspora. This background made me heavily rely on grassroots organizing. Hire the people that want to innovate, hire the changemakers, hire the button-pushers. Reduce reliance on whiteness as change. This will become natural for the organization and become organizational change. Change comes from us on the ground.”\\nA huge thank you to Arbeena Thapa for sharing her experiences, and being vulnerable with us. Your words were inspiring and the opportunity to understand your perspective more has been valuable. We hope we can become better support for the APIDA community as we learn and grow on our journey of cultivating inclusive growth.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = articles[0]['content']\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e70fe9",
   "metadata": {},
   "source": [
    "# Convert all text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b48256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may is traditionally known as asian american and pacific islander (aapi) heritage month. this month we celebrate the history and contributions made possible by our aapi friends, family, and community. we also examine our level of support and seek opportunities to better understand the aapi community.\n",
      "\n",
      "in an effort to address real concerns and experiences, we sat down with arbeena thapa, one of codeup’s financial aid and enrollment managers.\n",
      "arbeena identifies as nepali american and desi. arbeena’s parents immigrated to texas in 1988 for better employment and educational opportunities. arbeena’s older sister was five when they made the move to the us. arbeena was born later, becoming the first in her family to be a us citizen.\n",
      "at codeup we take our efforts at inclusivity very seriously. after speaking with arbeena, we were taught that the term aapi excludes desi-american individuals. hence, we will now use the term asian pacific islander desi american (apida).\n",
      "here is how the rest of our conversation with arbeena went!\n",
      "how do you celebrate or connect with your heritage and cultural traditions?\n",
      "“i celebrate nepal’s version of christmas or dashain. this is a nine-day celebration also known as dussehra. i grew up as hindu and i identify as hindu, this is a very large part of my heritage. “\n",
      "“other ways i connect with my culture include sharing food! momos are south asian dumplings and they’re my favorite to make and share.”\n",
      "“on my asian american side, i am an advocate of immigrant justice and erasure within apida social or political movements. i participate in events to embrace my identity such as immigrant justice advocacy because i come from a mixed-status family. i’ve always been in a community with undocumented asian immigrants. .”\n",
      "what are some of the challenges you have faced as an apida individual, personally or professionally?\n",
      "“i often struggle with being gendered as compliant or a pushover. professionally, i am often stereotyped as meek, so i’ve been overlooked for leadership roles. we are seen as perpetually foreign; people tend to other us in that way, yet put us on a pedestal for what a model minority looks like. this has made me hesitant to share my heritage in the past because these assumptions get mapped onto me. ”\n",
      "can you describe some common barriers of entry that apida individuals, specifically women may face when trying to enter or advance in the workplace?\n",
      "“being overlooked for leadership. in the past, i have not been viewed as a leader. people sometimes have preconceived stereotypes of asian women not being able to be bold, or being vocal can be mistaken for being too emotional. “\n",
      "how do you believe microaggressions impact apida individuals in the workplace? can you provide examples of such microaggressions?\n",
      "“erasure is big. to me, only saying ‘merry christmas’ isn’t inclusive to other religions. people are often resistant to saying ‘happy holidays,’ but saying merry christmas excludes, and does not appreciate my heritage. “\n",
      "“often microaggressions are not micro at all. they typically are not aggressive racialized violence, but the term ‘micro’ minimizes impact.”\n",
      "“some that i’ve heard are ‘what kind of asian are you?’ or ‘where are you from?’ this automatically makes me the ‘other’ and not seen as american. even within the apida community, south asians are overlooked as “asian”.”\n",
      "how important is representation, specifically apida representation, in organizational leadership positions?\n",
      "“i want to say that it is important to have someone who looks like you in leadership roles, and it is, but those leaders may not share the same beliefs as you. certain privileges such as wealth, resources, or lack of interaction with lower-socioeconomic-status asian americans may cause a difference in community politics. i do not think the bamboo ceiling is acceptable, but the company you work for plays a big part in your politics and belief alignment.”\n",
      "how do you feel about code-switching, and have you ever felt it necessary to code-switch?\n",
      "“i like sharing south asian terms or connecting with others that have similar heritage and culture. a workplace that is welcoming to going into this sort of breakout is refreshing and makes space for us. however, having to code-switch could also mean a workplace that is not conducive and welcoming of other cultures. “\n",
      "finally, in your opinion, what long-term strategies can create lasting change in the workplace and ensure support, equality, and inclusion for apida individuals?\n",
      "“prior to a career in financial aid, i did a lot of research related to the post-9/11 immigration of the south asian diaspora. this background made me heavily rely on grassroots organizing. hire the people that want to innovate, hire the changemakers, hire the button-pushers. reduce reliance on whiteness as change. this will become natural for the organization and become organizational change. change comes from us on the ground.”\n",
      "a huge thank you to arbeena thapa for sharing her experiences, and being vulnerable with us. your words were inspiring and the opportunity to understand your perspective more has been valuable. we hope we can become better support for the apida community as we learn and grow on our journey of cultivating inclusive growth.\n"
     ]
    }
   ],
   "source": [
    "article = article.lower()\n",
    "print(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045a381",
   "metadata": {},
   "source": [
    "# Removing accented characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac3869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may is traditionally known as asian american and pacific islander (aapi) heritage month. this month we celebrate the history and contributions made possible by our aapi friends, family, and community. we also examine our level of support and seek opportunities to better understand the aapi community.\n",
      "\n",
      "in an effort to address real concerns and experiences, we sat down with arbeena thapa, one of codeups financial aid and enrollment managers.\n",
      "arbeena identifies as nepali american and desi. arbeenas\n"
     ]
    }
   ],
   "source": [
    "article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "print(article[0:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0861058",
   "metadata": {},
   "source": [
    "# Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a30fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may is traditionally known as asian american and pacific islander aapi heritage month this month we celebrate the history and contributions made possible by our aapi friends family and community we also examine our level of support and seek opportunities to better understand the aapi community\n",
      "\n",
      "in an effort to address real concerns and experiences we sat down with arbeena thapa one of codeups financial aid and enrollment managers\n",
      "arbeena identifies as nepali american and desi arbeenas parents im\n"
     ]
    }
   ],
   "source": [
    "# remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "print(article[0:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8f97",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041d6733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may is traditionally known as asian american and pacific islander aapi heritage month this month we celebrate the history and contributions made possible by our aapi friends family and community we also examine our level of support and seek opportunities to better understand the aapi community\n",
      "\n",
      "in an effort to address real concerns and experiences we sat down with arbeena thapa one of codeups financial aid and enrollment managers\n",
      "arbeena identifies as nepali american and desi arbeenas parents im\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(article, return_str=True)[0:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba737d",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb347716",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce4d6f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call', 'call', 'call')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the nltk stemmer object, then use it\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "ps.stem('call'), ps.stem('called'), ps.stem('calling')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e97f3a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may',\n",
       " 'is',\n",
       " 'tradit',\n",
       " 'known',\n",
       " 'as',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'and',\n",
       " 'pacif',\n",
       " 'island',\n",
       " 'aapi',\n",
       " 'heritag',\n",
       " 'month',\n",
       " 'thi',\n",
       " 'month',\n",
       " 'we',\n",
       " 'celebr',\n",
       " 'the',\n",
       " 'histori',\n",
       " 'and',\n",
       " 'contribut',\n",
       " 'made',\n",
       " 'possibl',\n",
       " 'by',\n",
       " 'our',\n",
       " 'aapi',\n",
       " 'friend',\n",
       " 'famili',\n",
       " 'and',\n",
       " 'commun',\n",
       " 'we',\n",
       " 'also',\n",
       " 'examin',\n",
       " 'our',\n",
       " 'level',\n",
       " 'of',\n",
       " 'support',\n",
       " 'and',\n",
       " 'seek',\n",
       " 'opportun',\n",
       " 'to',\n",
       " 'better',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'aapi',\n",
       " 'commun',\n",
       " 'in',\n",
       " 'an',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'address',\n",
       " 'real',\n",
       " 'concern',\n",
       " 'and',\n",
       " 'experi',\n",
       " 'we',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'thapa',\n",
       " 'one',\n",
       " 'of',\n",
       " 'codeup',\n",
       " 'financi',\n",
       " 'aid',\n",
       " 'and',\n",
       " 'enrol',\n",
       " 'manag',\n",
       " 'arbeena',\n",
       " 'identifi',\n",
       " 'as',\n",
       " 'nepali',\n",
       " 'american',\n",
       " 'and',\n",
       " 'desi',\n",
       " 'arbeena',\n",
       " 'parent',\n",
       " 'immigr',\n",
       " 'to',\n",
       " 'texa',\n",
       " 'in',\n",
       " '1988',\n",
       " 'for',\n",
       " 'better',\n",
       " 'employ',\n",
       " 'and',\n",
       " 'educ',\n",
       " 'opportun',\n",
       " 'arbeena',\n",
       " 'older',\n",
       " 'sister',\n",
       " 'wa',\n",
       " 'five',\n",
       " 'when',\n",
       " 'they',\n",
       " 'made',\n",
       " 'the',\n",
       " 'move',\n",
       " 'to',\n",
       " 'the',\n",
       " 'us',\n",
       " 'arbeena',\n",
       " 'wa',\n",
       " 'born',\n",
       " 'later',\n",
       " 'becom',\n",
       " 'the',\n",
       " 'first',\n",
       " 'in',\n",
       " 'her',\n",
       " 'famili',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'us',\n",
       " 'citizen',\n",
       " 'at',\n",
       " 'codeup',\n",
       " 'we',\n",
       " 'take',\n",
       " 'our',\n",
       " 'effort',\n",
       " 'at',\n",
       " 'inclus',\n",
       " 'veri',\n",
       " 'serious',\n",
       " 'after',\n",
       " 'speak',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'we',\n",
       " 'were',\n",
       " 'taught',\n",
       " 'that',\n",
       " 'the',\n",
       " 'term',\n",
       " 'aapi',\n",
       " 'exclud',\n",
       " 'desiamerican',\n",
       " 'individu',\n",
       " 'henc',\n",
       " 'we',\n",
       " 'will',\n",
       " 'now',\n",
       " 'use',\n",
       " 'the',\n",
       " 'term',\n",
       " 'asian',\n",
       " 'pacif',\n",
       " 'island',\n",
       " 'desi',\n",
       " 'american',\n",
       " 'apida',\n",
       " 'here',\n",
       " 'is',\n",
       " 'how',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'our',\n",
       " 'convers',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'went',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'celebr',\n",
       " 'or',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'your',\n",
       " 'heritag',\n",
       " 'and',\n",
       " 'cultur',\n",
       " 'tradit',\n",
       " 'i',\n",
       " 'celebr',\n",
       " 'nepal',\n",
       " 'version',\n",
       " 'of',\n",
       " 'christma',\n",
       " 'or',\n",
       " 'dashain',\n",
       " 'thi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nineday',\n",
       " 'celebr',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'dussehra',\n",
       " 'i',\n",
       " 'grew',\n",
       " 'up',\n",
       " 'as',\n",
       " 'hindu',\n",
       " 'and',\n",
       " 'i',\n",
       " 'identifi',\n",
       " 'as',\n",
       " 'hindu',\n",
       " 'thi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'veri',\n",
       " 'larg',\n",
       " 'part',\n",
       " 'of',\n",
       " 'my',\n",
       " 'heritag',\n",
       " 'other',\n",
       " 'way',\n",
       " 'i',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'my',\n",
       " 'cultur',\n",
       " 'includ',\n",
       " 'share',\n",
       " 'food',\n",
       " 'momo',\n",
       " 'are',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'dumpl',\n",
       " 'and',\n",
       " 'theyr',\n",
       " 'my',\n",
       " 'favorit',\n",
       " 'to',\n",
       " 'make',\n",
       " 'and',\n",
       " 'share',\n",
       " 'on',\n",
       " 'my',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'side',\n",
       " 'i',\n",
       " 'am',\n",
       " 'an',\n",
       " 'advoc',\n",
       " 'of',\n",
       " 'immigr',\n",
       " 'justic',\n",
       " 'and',\n",
       " 'erasur',\n",
       " 'within',\n",
       " 'apida',\n",
       " 'social',\n",
       " 'or',\n",
       " 'polit',\n",
       " 'movement',\n",
       " 'i',\n",
       " 'particip',\n",
       " 'in',\n",
       " 'event',\n",
       " 'to',\n",
       " 'embrac',\n",
       " 'my',\n",
       " 'ident',\n",
       " 'such',\n",
       " 'as',\n",
       " 'immigr',\n",
       " 'justic',\n",
       " 'advocaci',\n",
       " 'becaus',\n",
       " 'i',\n",
       " 'come',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mixedstatu',\n",
       " 'famili',\n",
       " 'ive',\n",
       " 'alway',\n",
       " 'been',\n",
       " 'in',\n",
       " 'a',\n",
       " 'commun',\n",
       " 'with',\n",
       " 'undocu',\n",
       " 'asian',\n",
       " 'immigr',\n",
       " 'what',\n",
       " 'are',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'challeng',\n",
       " 'you',\n",
       " 'have',\n",
       " 'face',\n",
       " 'as',\n",
       " 'an',\n",
       " 'apida',\n",
       " 'individu',\n",
       " 'person',\n",
       " 'or',\n",
       " 'profession',\n",
       " 'i',\n",
       " 'often',\n",
       " 'struggl',\n",
       " 'with',\n",
       " 'be',\n",
       " 'gender',\n",
       " 'as',\n",
       " 'compliant',\n",
       " 'or',\n",
       " 'a',\n",
       " 'pushov',\n",
       " 'profession',\n",
       " 'i',\n",
       " 'am',\n",
       " 'often',\n",
       " 'stereotyp',\n",
       " 'as',\n",
       " 'meek',\n",
       " 'so',\n",
       " 'ive',\n",
       " 'been',\n",
       " 'overlook',\n",
       " 'for',\n",
       " 'leadership',\n",
       " 'role',\n",
       " 'we',\n",
       " 'are',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'perpetu',\n",
       " 'foreign',\n",
       " 'peopl',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'other',\n",
       " 'us',\n",
       " 'in',\n",
       " 'that',\n",
       " 'way',\n",
       " 'yet',\n",
       " 'put',\n",
       " 'us',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pedest',\n",
       " 'for',\n",
       " 'what',\n",
       " 'a',\n",
       " 'model',\n",
       " 'minor',\n",
       " 'look',\n",
       " 'like',\n",
       " 'thi',\n",
       " 'ha',\n",
       " 'made',\n",
       " 'me',\n",
       " 'hesit',\n",
       " 'to',\n",
       " 'share',\n",
       " 'my',\n",
       " 'heritag',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'becaus',\n",
       " 'these',\n",
       " 'assumpt',\n",
       " 'get',\n",
       " 'map',\n",
       " 'onto',\n",
       " 'me',\n",
       " 'can',\n",
       " 'you',\n",
       " 'describ',\n",
       " 'some',\n",
       " 'common',\n",
       " 'barrier',\n",
       " 'of',\n",
       " 'entri',\n",
       " 'that',\n",
       " 'apida',\n",
       " 'individu',\n",
       " 'specif',\n",
       " 'women',\n",
       " 'may',\n",
       " 'face',\n",
       " 'when',\n",
       " 'tri',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'or',\n",
       " 'advanc',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplac',\n",
       " 'be',\n",
       " 'overlook',\n",
       " 'for',\n",
       " 'leadership',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'i',\n",
       " 'have',\n",
       " 'not',\n",
       " 'been',\n",
       " 'view',\n",
       " 'as',\n",
       " 'a',\n",
       " 'leader',\n",
       " 'peopl',\n",
       " 'sometim',\n",
       " 'have',\n",
       " 'preconceiv',\n",
       " 'stereotyp',\n",
       " 'of',\n",
       " 'asian',\n",
       " 'women',\n",
       " 'not',\n",
       " 'be',\n",
       " 'abl',\n",
       " 'to',\n",
       " 'be',\n",
       " 'bold',\n",
       " 'or',\n",
       " 'be',\n",
       " 'vocal',\n",
       " 'can',\n",
       " 'be',\n",
       " 'mistaken',\n",
       " 'for',\n",
       " 'be',\n",
       " 'too',\n",
       " 'emot',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'believ',\n",
       " 'microaggress',\n",
       " 'impact',\n",
       " 'apida',\n",
       " 'individu',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplac',\n",
       " 'can',\n",
       " 'you',\n",
       " 'provid',\n",
       " 'exampl',\n",
       " 'of',\n",
       " 'such',\n",
       " 'microaggress',\n",
       " 'erasur',\n",
       " 'is',\n",
       " 'big',\n",
       " 'to',\n",
       " 'me',\n",
       " 'onli',\n",
       " 'say',\n",
       " 'merri',\n",
       " 'christma',\n",
       " 'isnt',\n",
       " 'inclus',\n",
       " 'to',\n",
       " 'other',\n",
       " 'religion',\n",
       " 'peopl',\n",
       " 'are',\n",
       " 'often',\n",
       " 'resist',\n",
       " 'to',\n",
       " 'say',\n",
       " 'happi',\n",
       " 'holiday',\n",
       " 'but',\n",
       " 'say',\n",
       " 'merri',\n",
       " 'christma',\n",
       " 'exclud',\n",
       " 'and',\n",
       " 'doe',\n",
       " 'not',\n",
       " 'appreci',\n",
       " 'my',\n",
       " 'heritag',\n",
       " 'often',\n",
       " 'microaggress',\n",
       " 'are',\n",
       " 'not',\n",
       " 'micro',\n",
       " 'at',\n",
       " 'all',\n",
       " 'they',\n",
       " 'typic',\n",
       " 'are',\n",
       " 'not',\n",
       " 'aggress',\n",
       " 'racial',\n",
       " 'violenc',\n",
       " 'but',\n",
       " 'the',\n",
       " 'term',\n",
       " 'micro',\n",
       " 'minim',\n",
       " 'impact',\n",
       " 'some',\n",
       " 'that',\n",
       " 'ive',\n",
       " 'heard',\n",
       " 'are',\n",
       " 'what',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'asian',\n",
       " 'are',\n",
       " 'you',\n",
       " 'or',\n",
       " 'where',\n",
       " 'are',\n",
       " 'you',\n",
       " 'from',\n",
       " 'thi',\n",
       " 'automat',\n",
       " 'make',\n",
       " 'me',\n",
       " 'the',\n",
       " 'other',\n",
       " 'and',\n",
       " 'not',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'american',\n",
       " 'even',\n",
       " 'within',\n",
       " 'the',\n",
       " 'apida',\n",
       " 'commun',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'are',\n",
       " 'overlook',\n",
       " 'as',\n",
       " 'asian',\n",
       " 'how',\n",
       " 'import',\n",
       " 'is',\n",
       " 'represent',\n",
       " 'specif',\n",
       " 'apida',\n",
       " 'represent',\n",
       " 'in',\n",
       " 'organiz',\n",
       " 'leadership',\n",
       " 'posit',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'import',\n",
       " 'to',\n",
       " 'have',\n",
       " 'someon',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'you',\n",
       " 'in',\n",
       " 'leadership',\n",
       " 'role',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'but',\n",
       " 'those',\n",
       " 'leader',\n",
       " 'may',\n",
       " 'not',\n",
       " 'share',\n",
       " 'the',\n",
       " 'same',\n",
       " 'belief',\n",
       " 'as',\n",
       " 'you',\n",
       " 'certain',\n",
       " 'privileg',\n",
       " 'such',\n",
       " 'as',\n",
       " 'wealth',\n",
       " 'resourc',\n",
       " 'or',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'interact',\n",
       " 'with',\n",
       " 'lowersocioeconomicstatu',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'may',\n",
       " 'caus',\n",
       " 'a',\n",
       " 'differ',\n",
       " 'in',\n",
       " 'commun',\n",
       " 'polit',\n",
       " 'i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'think',\n",
       " 'the',\n",
       " 'bamboo',\n",
       " 'ceil',\n",
       " 'is',\n",
       " 'accept',\n",
       " 'but',\n",
       " 'the',\n",
       " 'compani',\n",
       " 'you',\n",
       " 'work',\n",
       " 'for',\n",
       " 'play',\n",
       " 'a',\n",
       " 'big',\n",
       " 'part',\n",
       " 'in',\n",
       " 'your',\n",
       " 'polit',\n",
       " 'and',\n",
       " 'belief',\n",
       " 'align',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'about',\n",
       " 'codeswitch',\n",
       " 'and',\n",
       " 'have',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'felt',\n",
       " 'it',\n",
       " 'necessari',\n",
       " 'to',\n",
       " 'codeswitch',\n",
       " 'i',\n",
       " 'like',\n",
       " 'share',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'term',\n",
       " 'or',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'other',\n",
       " 'that',\n",
       " 'have',\n",
       " 'similar',\n",
       " 'heritag',\n",
       " 'and',\n",
       " 'cultur',\n",
       " 'a',\n",
       " 'workplac',\n",
       " 'that',\n",
       " 'is',\n",
       " 'welcom',\n",
       " 'to',\n",
       " 'go',\n",
       " 'into',\n",
       " 'thi',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'breakout',\n",
       " 'is',\n",
       " 'refresh',\n",
       " 'and',\n",
       " 'make',\n",
       " 'space',\n",
       " 'for',\n",
       " 'us',\n",
       " 'howev',\n",
       " 'have',\n",
       " 'to',\n",
       " 'codeswitch',\n",
       " 'could',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'a',\n",
       " 'workplac',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'conduc',\n",
       " 'and',\n",
       " 'welcom',\n",
       " 'of',\n",
       " 'other',\n",
       " 'cultur',\n",
       " 'final',\n",
       " 'in',\n",
       " 'your',\n",
       " 'opinion',\n",
       " 'what',\n",
       " 'longterm',\n",
       " 'strategi',\n",
       " 'can',\n",
       " 'creat',\n",
       " 'last',\n",
       " 'chang',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplac',\n",
       " 'and',\n",
       " 'ensur',\n",
       " 'support',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inclus',\n",
       " 'for',\n",
       " 'apida',\n",
       " 'individu',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'a',\n",
       " 'career',\n",
       " 'in',\n",
       " 'financi',\n",
       " 'aid',\n",
       " 'i',\n",
       " 'did',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'research',\n",
       " 'relat',\n",
       " 'to',\n",
       " 'the',\n",
       " 'post911',\n",
       " 'immigr',\n",
       " 'of',\n",
       " 'the',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'diaspora',\n",
       " 'thi',\n",
       " 'background',\n",
       " 'made',\n",
       " 'me',\n",
       " 'heavili',\n",
       " 'reli',\n",
       " 'on',\n",
       " 'grassroot',\n",
       " 'organ',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'peopl',\n",
       " 'that',\n",
       " 'want',\n",
       " 'to',\n",
       " 'innov',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'changemak',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'buttonpush',\n",
       " 'reduc',\n",
       " 'relianc',\n",
       " 'on',\n",
       " 'white',\n",
       " 'as',\n",
       " 'chang',\n",
       " 'thi',\n",
       " 'will',\n",
       " 'becom',\n",
       " 'natur',\n",
       " 'for',\n",
       " 'the',\n",
       " 'organ',\n",
       " 'and',\n",
       " 'becom',\n",
       " 'organiz',\n",
       " 'chang',\n",
       " 'chang',\n",
       " 'come',\n",
       " 'from',\n",
       " 'us',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'to',\n",
       " 'arbeena',\n",
       " 'thapa',\n",
       " 'for',\n",
       " 'share',\n",
       " 'her',\n",
       " 'experi',\n",
       " 'and',\n",
       " 'be',\n",
       " 'vulner',\n",
       " 'with',\n",
       " 'us',\n",
       " 'your',\n",
       " 'word',\n",
       " 'were',\n",
       " 'inspir',\n",
       " 'and',\n",
       " 'the',\n",
       " 'opportun',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'your',\n",
       " 'perspect',\n",
       " 'more',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'valuabl',\n",
       " 'we',\n",
       " 'hope',\n",
       " 'we',\n",
       " 'can',\n",
       " 'becom',\n",
       " 'better',\n",
       " 'support',\n",
       " 'for',\n",
       " 'the',\n",
       " 'apida',\n",
       " 'commun',\n",
       " 'as',\n",
       " 'we',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'grow',\n",
       " 'on',\n",
       " 'our',\n",
       " 'journey',\n",
       " 'of',\n",
       " 'cultiv',\n",
       " 'inclus',\n",
       " 'growth']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the stemminziner to all the article\n",
    "stems = [ps.stem(word) for word in article.split()]\n",
    "\n",
    "# Joining the stemmed words for illustration purposes\n",
    "article_stemmed = ' '.join(stems)\n",
    "\n",
    "# Displaying the words in a list\n",
    "stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3bfa018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the      29\n",
       "and      27\n",
       "to       24\n",
       "as       17\n",
       "in       17\n",
       "of       17\n",
       "a        16\n",
       "i        14\n",
       "you      13\n",
       "asian    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts for all the words\n",
    "pd.Series(stems).value_counts().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5102d798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may',\n",
       " 'is',\n",
       " 'tradit',\n",
       " 'known',\n",
       " 'as',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'and',\n",
       " 'pacif',\n",
       " 'island',\n",
       " 'aapi',\n",
       " 'heritag',\n",
       " 'month',\n",
       " 'thi',\n",
       " 'month',\n",
       " 'we',\n",
       " 'celebr',\n",
       " 'the',\n",
       " 'histori',\n",
       " 'and',\n",
       " 'contribut',\n",
       " 'made',\n",
       " 'possibl',\n",
       " 'by',\n",
       " 'our',\n",
       " 'aapi',\n",
       " 'friend',\n",
       " 'famili',\n",
       " 'and',\n",
       " 'commun',\n",
       " 'we',\n",
       " 'also',\n",
       " 'examin',\n",
       " 'our',\n",
       " 'level',\n",
       " 'of',\n",
       " 'support',\n",
       " 'and',\n",
       " 'seek',\n",
       " 'opportun',\n",
       " 'to',\n",
       " 'better',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'aapi',\n",
       " 'commun',\n",
       " 'in',\n",
       " 'an',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'address',\n",
       " 'real',\n",
       " 'concern',\n",
       " 'and',\n",
       " 'experi',\n",
       " 'we',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'thapa',\n",
       " 'one',\n",
       " 'of',\n",
       " 'codeup',\n",
       " 'financi',\n",
       " 'aid',\n",
       " 'and',\n",
       " 'enrol',\n",
       " 'manag',\n",
       " 'arbeena',\n",
       " 'identifi',\n",
       " 'as',\n",
       " 'nepali',\n",
       " 'american',\n",
       " 'and',\n",
       " 'desi',\n",
       " 'arbeena',\n",
       " 'parent',\n",
       " 'immigr',\n",
       " 'to',\n",
       " 'texa',\n",
       " 'in',\n",
       " '1988',\n",
       " 'for',\n",
       " 'better',\n",
       " 'employ',\n",
       " 'and',\n",
       " 'educ',\n",
       " 'opportun',\n",
       " 'arbeena',\n",
       " 'older',\n",
       " 'sister',\n",
       " 'wa',\n",
       " 'five',\n",
       " 'when',\n",
       " 'they',\n",
       " 'made',\n",
       " 'the',\n",
       " 'move',\n",
       " 'to',\n",
       " 'the',\n",
       " 'us',\n",
       " 'arbeena',\n",
       " 'wa',\n",
       " 'born',\n",
       " 'later',\n",
       " 'becom',\n",
       " 'the',\n",
       " 'first',\n",
       " 'in',\n",
       " 'her',\n",
       " 'famili',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'us',\n",
       " 'citizen',\n",
       " 'at',\n",
       " 'codeup',\n",
       " 'we',\n",
       " 'take',\n",
       " 'our',\n",
       " 'effort',\n",
       " 'at',\n",
       " 'inclus',\n",
       " 'veri',\n",
       " 'serious',\n",
       " 'after',\n",
       " 'speak',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'we',\n",
       " 'were',\n",
       " 'taught',\n",
       " 'that',\n",
       " 'the',\n",
       " 'term',\n",
       " 'aapi',\n",
       " 'exclud',\n",
       " 'desiamerican',\n",
       " 'individu',\n",
       " 'henc',\n",
       " 'we',\n",
       " 'will',\n",
       " 'now',\n",
       " 'use',\n",
       " 'the',\n",
       " 'term',\n",
       " 'asian',\n",
       " 'pacif',\n",
       " 'island',\n",
       " 'desi',\n",
       " 'american',\n",
       " 'apida',\n",
       " 'here',\n",
       " 'is',\n",
       " 'how',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'our',\n",
       " 'convers',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'went',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'celebr',\n",
       " 'or',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'your',\n",
       " 'heritag',\n",
       " 'and',\n",
       " 'cultur',\n",
       " 'tradit',\n",
       " 'i',\n",
       " 'celebr',\n",
       " 'nepal',\n",
       " 'version',\n",
       " 'of',\n",
       " 'christma',\n",
       " 'or',\n",
       " 'dashain',\n",
       " 'thi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nineday',\n",
       " 'celebr',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'dussehra',\n",
       " 'i',\n",
       " 'grew',\n",
       " 'up',\n",
       " 'as',\n",
       " 'hindu',\n",
       " 'and',\n",
       " 'i',\n",
       " 'identifi',\n",
       " 'as',\n",
       " 'hindu',\n",
       " 'thi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'veri',\n",
       " 'larg',\n",
       " 'part',\n",
       " 'of',\n",
       " 'my',\n",
       " 'heritag',\n",
       " 'other',\n",
       " 'way',\n",
       " 'i',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'my',\n",
       " 'cultur',\n",
       " 'includ',\n",
       " 'share',\n",
       " 'food',\n",
       " 'momo',\n",
       " 'are',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'dumpl',\n",
       " 'and',\n",
       " 'theyr',\n",
       " 'my',\n",
       " 'favorit',\n",
       " 'to',\n",
       " 'make',\n",
       " 'and',\n",
       " 'share',\n",
       " 'on',\n",
       " 'my',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'side',\n",
       " 'i',\n",
       " 'am',\n",
       " 'an',\n",
       " 'advoc',\n",
       " 'of',\n",
       " 'immigr',\n",
       " 'justic',\n",
       " 'and',\n",
       " 'erasur',\n",
       " 'within',\n",
       " 'apida',\n",
       " 'social',\n",
       " 'or',\n",
       " 'polit',\n",
       " 'movement',\n",
       " 'i',\n",
       " 'particip',\n",
       " 'in',\n",
       " 'event',\n",
       " 'to',\n",
       " 'embrac',\n",
       " 'my',\n",
       " 'ident',\n",
       " 'such',\n",
       " 'as',\n",
       " 'immigr',\n",
       " 'justic',\n",
       " 'advocaci',\n",
       " 'becaus',\n",
       " 'i',\n",
       " 'come',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mixedstatu',\n",
       " 'famili',\n",
       " 'ive',\n",
       " 'alway',\n",
       " 'been',\n",
       " 'in',\n",
       " 'a',\n",
       " 'commun',\n",
       " 'with',\n",
       " 'undocu',\n",
       " 'asian',\n",
       " 'immigr',\n",
       " 'what',\n",
       " 'are',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'challeng',\n",
       " 'you',\n",
       " 'have',\n",
       " 'face',\n",
       " 'as',\n",
       " 'an',\n",
       " 'apida',\n",
       " 'individu',\n",
       " 'person',\n",
       " 'or',\n",
       " 'profession',\n",
       " 'i',\n",
       " 'often',\n",
       " 'struggl',\n",
       " 'with',\n",
       " 'be',\n",
       " 'gender',\n",
       " 'as',\n",
       " 'compliant',\n",
       " 'or',\n",
       " 'a',\n",
       " 'pushov',\n",
       " 'profession',\n",
       " 'i',\n",
       " 'am',\n",
       " 'often',\n",
       " 'stereotyp',\n",
       " 'as',\n",
       " 'meek',\n",
       " 'so',\n",
       " 'ive',\n",
       " 'been',\n",
       " 'overlook',\n",
       " 'for',\n",
       " 'leadership',\n",
       " 'role',\n",
       " 'we',\n",
       " 'are',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'perpetu',\n",
       " 'foreign',\n",
       " 'peopl',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'other',\n",
       " 'us',\n",
       " 'in',\n",
       " 'that',\n",
       " 'way',\n",
       " 'yet',\n",
       " 'put',\n",
       " 'us',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pedest',\n",
       " 'for',\n",
       " 'what',\n",
       " 'a',\n",
       " 'model',\n",
       " 'minor',\n",
       " 'look',\n",
       " 'like',\n",
       " 'thi',\n",
       " 'ha',\n",
       " 'made',\n",
       " 'me',\n",
       " 'hesit',\n",
       " 'to',\n",
       " 'share',\n",
       " 'my',\n",
       " 'heritag',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'becaus',\n",
       " 'these',\n",
       " 'assumpt',\n",
       " 'get',\n",
       " 'map',\n",
       " 'onto',\n",
       " 'me',\n",
       " 'can',\n",
       " 'you',\n",
       " 'describ',\n",
       " 'some',\n",
       " 'common',\n",
       " 'barrier',\n",
       " 'of',\n",
       " 'entri',\n",
       " 'that',\n",
       " 'apida',\n",
       " 'individu',\n",
       " 'specif',\n",
       " 'women',\n",
       " 'may',\n",
       " 'face',\n",
       " 'when',\n",
       " 'tri',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'or',\n",
       " 'advanc',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplac',\n",
       " 'be',\n",
       " 'overlook',\n",
       " 'for',\n",
       " 'leadership',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'i',\n",
       " 'have',\n",
       " 'not',\n",
       " 'been',\n",
       " 'view',\n",
       " 'as',\n",
       " 'a',\n",
       " 'leader',\n",
       " 'peopl',\n",
       " 'sometim',\n",
       " 'have',\n",
       " 'preconceiv',\n",
       " 'stereotyp',\n",
       " 'of',\n",
       " 'asian',\n",
       " 'women',\n",
       " 'not',\n",
       " 'be',\n",
       " 'abl',\n",
       " 'to',\n",
       " 'be',\n",
       " 'bold',\n",
       " 'or',\n",
       " 'be',\n",
       " 'vocal',\n",
       " 'can',\n",
       " 'be',\n",
       " 'mistaken',\n",
       " 'for',\n",
       " 'be',\n",
       " 'too',\n",
       " 'emot',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'believ',\n",
       " 'microaggress',\n",
       " 'impact',\n",
       " 'apida',\n",
       " 'individu',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplac',\n",
       " 'can',\n",
       " 'you',\n",
       " 'provid',\n",
       " 'exampl',\n",
       " 'of',\n",
       " 'such',\n",
       " 'microaggress',\n",
       " 'erasur',\n",
       " 'is',\n",
       " 'big',\n",
       " 'to',\n",
       " 'me',\n",
       " 'onli',\n",
       " 'say',\n",
       " 'merri',\n",
       " 'christma',\n",
       " 'isnt',\n",
       " 'inclus',\n",
       " 'to',\n",
       " 'other',\n",
       " 'religion',\n",
       " 'peopl',\n",
       " 'are',\n",
       " 'often',\n",
       " 'resist',\n",
       " 'to',\n",
       " 'say',\n",
       " 'happi',\n",
       " 'holiday',\n",
       " 'but',\n",
       " 'say',\n",
       " 'merri',\n",
       " 'christma',\n",
       " 'exclud',\n",
       " 'and',\n",
       " 'doe',\n",
       " 'not',\n",
       " 'appreci',\n",
       " 'my',\n",
       " 'heritag',\n",
       " 'often',\n",
       " 'microaggress',\n",
       " 'are',\n",
       " 'not',\n",
       " 'micro',\n",
       " 'at',\n",
       " 'all',\n",
       " 'they',\n",
       " 'typic',\n",
       " 'are',\n",
       " 'not',\n",
       " 'aggress',\n",
       " 'racial',\n",
       " 'violenc',\n",
       " 'but',\n",
       " 'the',\n",
       " 'term',\n",
       " 'micro',\n",
       " 'minim',\n",
       " 'impact',\n",
       " 'some',\n",
       " 'that',\n",
       " 'ive',\n",
       " 'heard',\n",
       " 'are',\n",
       " 'what',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'asian',\n",
       " 'are',\n",
       " 'you',\n",
       " 'or',\n",
       " 'where',\n",
       " 'are',\n",
       " 'you',\n",
       " 'from',\n",
       " 'thi',\n",
       " 'automat',\n",
       " 'make',\n",
       " 'me',\n",
       " 'the',\n",
       " 'other',\n",
       " 'and',\n",
       " 'not',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'american',\n",
       " 'even',\n",
       " 'within',\n",
       " 'the',\n",
       " 'apida',\n",
       " 'commun',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'are',\n",
       " 'overlook',\n",
       " 'as',\n",
       " 'asian',\n",
       " 'how',\n",
       " 'import',\n",
       " 'is',\n",
       " 'represent',\n",
       " 'specif',\n",
       " 'apida',\n",
       " 'represent',\n",
       " 'in',\n",
       " 'organiz',\n",
       " 'leadership',\n",
       " 'posit',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'import',\n",
       " 'to',\n",
       " 'have',\n",
       " 'someon',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'you',\n",
       " 'in',\n",
       " 'leadership',\n",
       " 'role',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'but',\n",
       " 'those',\n",
       " 'leader',\n",
       " 'may',\n",
       " 'not',\n",
       " 'share',\n",
       " 'the',\n",
       " 'same',\n",
       " 'belief',\n",
       " 'as',\n",
       " 'you',\n",
       " 'certain',\n",
       " 'privileg',\n",
       " 'such',\n",
       " 'as',\n",
       " 'wealth',\n",
       " 'resourc',\n",
       " 'or',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'interact',\n",
       " 'with',\n",
       " 'lowersocioeconomicstatu',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'may',\n",
       " 'caus',\n",
       " 'a',\n",
       " 'differ',\n",
       " 'in',\n",
       " 'commun',\n",
       " 'polit',\n",
       " 'i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'think',\n",
       " 'the',\n",
       " 'bamboo',\n",
       " 'ceil',\n",
       " 'is',\n",
       " 'accept',\n",
       " 'but',\n",
       " 'the',\n",
       " 'compani',\n",
       " 'you',\n",
       " 'work',\n",
       " 'for',\n",
       " 'play',\n",
       " 'a',\n",
       " 'big',\n",
       " 'part',\n",
       " 'in',\n",
       " 'your',\n",
       " 'polit',\n",
       " 'and',\n",
       " 'belief',\n",
       " 'align',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'about',\n",
       " 'codeswitch',\n",
       " 'and',\n",
       " 'have',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'felt',\n",
       " 'it',\n",
       " 'necessari',\n",
       " 'to',\n",
       " 'codeswitch',\n",
       " 'i',\n",
       " 'like',\n",
       " 'share',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'term',\n",
       " 'or',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'other',\n",
       " 'that',\n",
       " 'have',\n",
       " 'similar',\n",
       " 'heritag',\n",
       " 'and',\n",
       " 'cultur',\n",
       " 'a',\n",
       " 'workplac',\n",
       " 'that',\n",
       " 'is',\n",
       " 'welcom',\n",
       " 'to',\n",
       " 'go',\n",
       " 'into',\n",
       " 'thi',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'breakout',\n",
       " 'is',\n",
       " 'refresh',\n",
       " 'and',\n",
       " 'make',\n",
       " 'space',\n",
       " 'for',\n",
       " 'us',\n",
       " 'howev',\n",
       " 'have',\n",
       " 'to',\n",
       " 'codeswitch',\n",
       " 'could',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'a',\n",
       " 'workplac',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'conduc',\n",
       " 'and',\n",
       " 'welcom',\n",
       " 'of',\n",
       " 'other',\n",
       " 'cultur',\n",
       " 'final',\n",
       " 'in',\n",
       " 'your',\n",
       " 'opinion',\n",
       " 'what',\n",
       " 'longterm',\n",
       " 'strategi',\n",
       " 'can',\n",
       " 'creat',\n",
       " 'last',\n",
       " 'chang',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplac',\n",
       " 'and',\n",
       " 'ensur',\n",
       " 'support',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inclus',\n",
       " 'for',\n",
       " 'apida',\n",
       " 'individu',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'a',\n",
       " 'career',\n",
       " 'in',\n",
       " 'financi',\n",
       " 'aid',\n",
       " 'i',\n",
       " 'did',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'research',\n",
       " 'relat',\n",
       " 'to',\n",
       " 'the',\n",
       " 'post911',\n",
       " 'immigr',\n",
       " 'of',\n",
       " 'the',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'diaspora',\n",
       " 'thi',\n",
       " 'background',\n",
       " 'made',\n",
       " 'me',\n",
       " 'heavili',\n",
       " 'reli',\n",
       " 'on',\n",
       " 'grassroot',\n",
       " 'organ',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'peopl',\n",
       " 'that',\n",
       " 'want',\n",
       " 'to',\n",
       " 'innov',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'changemak',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'buttonpush',\n",
       " 'reduc',\n",
       " 'relianc',\n",
       " 'on',\n",
       " 'white',\n",
       " 'as',\n",
       " 'chang',\n",
       " 'thi',\n",
       " 'will',\n",
       " 'becom',\n",
       " 'natur',\n",
       " 'for',\n",
       " 'the',\n",
       " 'organ',\n",
       " 'and',\n",
       " 'becom',\n",
       " 'organiz',\n",
       " 'chang',\n",
       " 'chang',\n",
       " 'come',\n",
       " 'from',\n",
       " 'us',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'to',\n",
       " 'arbeena',\n",
       " 'thapa',\n",
       " 'for',\n",
       " 'share',\n",
       " 'her',\n",
       " 'experi',\n",
       " 'and',\n",
       " 'be',\n",
       " 'vulner',\n",
       " 'with',\n",
       " 'us',\n",
       " 'your',\n",
       " 'word',\n",
       " 'were',\n",
       " 'inspir',\n",
       " 'and',\n",
       " 'the',\n",
       " 'opportun',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'your',\n",
       " 'perspect',\n",
       " 'more',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'valuabl',\n",
       " 'we',\n",
       " 'hope',\n",
       " 'we',\n",
       " 'can',\n",
       " 'becom',\n",
       " 'better',\n",
       " 'support',\n",
       " 'for',\n",
       " 'the',\n",
       " 'apida',\n",
       " 'commun',\n",
       " 'as',\n",
       " 'we',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'grow',\n",
       " 'on',\n",
       " 'our',\n",
       " 'journey',\n",
       " 'of',\n",
       " 'cultiv',\n",
       " 'inclus',\n",
       " 'growth']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bf19e",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06aea3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/emanuelvilla/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################## Download only once ########################\n",
    "# import nltk\n",
    "# nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beb2d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Lemmatizer\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11019a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: may -- lemma: may\n",
      "stem: is -- lemma: is\n",
      "stem: tradit -- lemma: traditionally\n",
      "stem: known -- lemma: known\n",
      "stem: as -- lemma: a\n",
      "stem: asian -- lemma: asian\n",
      "stem: american -- lemma: american\n",
      "stem: and -- lemma: and\n",
      "stem: pacif -- lemma: pacific\n",
      "stem: island -- lemma: islander\n",
      "stem: aapi -- lemma: aapi\n",
      "stem: heritag -- lemma: heritage\n",
      "stem: month -- lemma: month\n",
      "stem: thi -- lemma: this\n",
      "stem: month -- lemma: month\n",
      "stem: we -- lemma: we\n",
      "stem: celebr -- lemma: celebrate\n",
      "stem: the -- lemma: the\n",
      "stem: histori -- lemma: history\n",
      "stem: and -- lemma: and\n",
      "stem: contribut -- lemma: contribution\n",
      "stem: made -- lemma: made\n",
      "stem: possibl -- lemma: possible\n",
      "stem: by -- lemma: by\n",
      "stem: our -- lemma: our\n",
      "stem: aapi -- lemma: aapi\n",
      "stem: friend -- lemma: friend\n",
      "stem: famili -- lemma: family\n",
      "stem: and -- lemma: and\n",
      "stem: commun -- lemma: community\n",
      "stem: we -- lemma: we\n",
      "stem: also -- lemma: also\n",
      "stem: examin -- lemma: examine\n",
      "stem: our -- lemma: our\n",
      "stem: level -- lemma: level\n",
      "stem: of -- lemma: of\n",
      "stem: support -- lemma: support\n",
      "stem: and -- lemma: and\n",
      "stem: seek -- lemma: seek\n",
      "stem: opportun -- lemma: opportunity\n",
      "stem: to -- lemma: to\n",
      "stem: better -- lemma: better\n",
      "stem: understand -- lemma: understand\n",
      "stem: the -- lemma: the\n",
      "stem: aapi -- lemma: aapi\n",
      "stem: commun -- lemma: community\n",
      "stem: in -- lemma: in\n",
      "stem: an -- lemma: an\n",
      "stem: effort -- lemma: effort\n",
      "stem: to -- lemma: to\n",
      "stem: address -- lemma: address\n",
      "stem: real -- lemma: real\n",
      "stem: concern -- lemma: concern\n",
      "stem: and -- lemma: and\n",
      "stem: experi -- lemma: experience\n",
      "stem: we -- lemma: we\n",
      "stem: sat -- lemma: sat\n",
      "stem: down -- lemma: down\n",
      "stem: with -- lemma: with\n",
      "stem: arbeena -- lemma: arbeena\n",
      "stem: thapa -- lemma: thapa\n",
      "stem: one -- lemma: one\n",
      "stem: of -- lemma: of\n",
      "stem: codeup -- lemma: codeups\n",
      "stem: financi -- lemma: financial\n",
      "stem: aid -- lemma: aid\n",
      "stem: and -- lemma: and\n",
      "stem: enrol -- lemma: enrollment\n",
      "stem: manag -- lemma: manager\n",
      "stem: arbeena -- lemma: arbeena\n",
      "stem: identifi -- lemma: identifies\n",
      "stem: as -- lemma: a\n",
      "stem: nepali -- lemma: nepali\n",
      "stem: american -- lemma: american\n",
      "stem: and -- lemma: and\n",
      "stem: desi -- lemma: desi\n",
      "stem: arbeena -- lemma: arbeenas\n",
      "stem: parent -- lemma: parent\n",
      "stem: immigr -- lemma: immigrated\n",
      "stem: to -- lemma: to\n",
      "stem: texa -- lemma: texas\n",
      "stem: in -- lemma: in\n",
      "stem: 1988 -- lemma: 1988\n",
      "stem: for -- lemma: for\n",
      "stem: better -- lemma: better\n",
      "stem: employ -- lemma: employment\n",
      "stem: and -- lemma: and\n",
      "stem: educ -- lemma: educational\n",
      "stem: opportun -- lemma: opportunity\n",
      "stem: arbeena -- lemma: arbeenas\n",
      "stem: older -- lemma: older\n",
      "stem: sister -- lemma: sister\n",
      "stem: wa -- lemma: wa\n",
      "stem: five -- lemma: five\n",
      "stem: when -- lemma: when\n",
      "stem: they -- lemma: they\n",
      "stem: made -- lemma: made\n",
      "stem: the -- lemma: the\n",
      "stem: move -- lemma: move\n",
      "stem: to -- lemma: to\n",
      "stem: the -- lemma: the\n",
      "stem: us -- lemma: u\n",
      "stem: arbeena -- lemma: arbeena\n",
      "stem: wa -- lemma: wa\n",
      "stem: born -- lemma: born\n",
      "stem: later -- lemma: later\n",
      "stem: becom -- lemma: becoming\n",
      "stem: the -- lemma: the\n",
      "stem: first -- lemma: first\n",
      "stem: in -- lemma: in\n",
      "stem: her -- lemma: her\n",
      "stem: famili -- lemma: family\n",
      "stem: to -- lemma: to\n",
      "stem: be -- lemma: be\n",
      "stem: a -- lemma: a\n",
      "stem: us -- lemma: u\n",
      "stem: citizen -- lemma: citizen\n",
      "stem: at -- lemma: at\n",
      "stem: codeup -- lemma: codeup\n",
      "stem: we -- lemma: we\n",
      "stem: take -- lemma: take\n",
      "stem: our -- lemma: our\n",
      "stem: effort -- lemma: effort\n",
      "stem: at -- lemma: at\n",
      "stem: inclus -- lemma: inclusivity\n",
      "stem: veri -- lemma: very\n",
      "stem: serious -- lemma: seriously\n",
      "stem: after -- lemma: after\n",
      "stem: speak -- lemma: speaking\n",
      "stem: with -- lemma: with\n",
      "stem: arbeena -- lemma: arbeena\n",
      "stem: we -- lemma: we\n",
      "stem: were -- lemma: were\n",
      "stem: taught -- lemma: taught\n",
      "stem: that -- lemma: that\n",
      "stem: the -- lemma: the\n",
      "stem: term -- lemma: term\n",
      "stem: aapi -- lemma: aapi\n",
      "stem: exclud -- lemma: excludes\n",
      "stem: desiamerican -- lemma: desiamerican\n",
      "stem: individu -- lemma: individual\n",
      "stem: henc -- lemma: hence\n",
      "stem: we -- lemma: we\n",
      "stem: will -- lemma: will\n",
      "stem: now -- lemma: now\n",
      "stem: use -- lemma: use\n",
      "stem: the -- lemma: the\n",
      "stem: term -- lemma: term\n",
      "stem: asian -- lemma: asian\n",
      "stem: pacif -- lemma: pacific\n",
      "stem: island -- lemma: islander\n",
      "stem: desi -- lemma: desi\n",
      "stem: american -- lemma: american\n",
      "stem: apida -- lemma: apida\n",
      "stem: here -- lemma: here\n",
      "stem: is -- lemma: is\n",
      "stem: how -- lemma: how\n",
      "stem: the -- lemma: the\n",
      "stem: rest -- lemma: rest\n",
      "stem: of -- lemma: of\n",
      "stem: our -- lemma: our\n",
      "stem: convers -- lemma: conversation\n",
      "stem: with -- lemma: with\n",
      "stem: arbeena -- lemma: arbeena\n",
      "stem: went -- lemma: went\n",
      "stem: how -- lemma: how\n",
      "stem: do -- lemma: do\n",
      "stem: you -- lemma: you\n",
      "stem: celebr -- lemma: celebrate\n",
      "stem: or -- lemma: or\n",
      "stem: connect -- lemma: connect\n",
      "stem: with -- lemma: with\n",
      "stem: your -- lemma: your\n",
      "stem: heritag -- lemma: heritage\n",
      "stem: and -- lemma: and\n",
      "stem: cultur -- lemma: cultural\n",
      "stem: tradit -- lemma: tradition\n",
      "stem: i -- lemma: i\n",
      "stem: celebr -- lemma: celebrate\n",
      "stem: nepal -- lemma: nepal\n",
      "stem: version -- lemma: version\n",
      "stem: of -- lemma: of\n",
      "stem: christma -- lemma: christmas\n",
      "stem: or -- lemma: or\n",
      "stem: dashain -- lemma: dashain\n",
      "stem: thi -- lemma: this\n",
      "stem: is -- lemma: is\n",
      "stem: a -- lemma: a\n",
      "stem: nineday -- lemma: nineday\n",
      "stem: celebr -- lemma: celebration\n",
      "stem: also -- lemma: also\n",
      "stem: known -- lemma: known\n",
      "stem: as -- lemma: a\n",
      "stem: dussehra -- lemma: dussehra\n",
      "stem: i -- lemma: i\n",
      "stem: grew -- lemma: grew\n",
      "stem: up -- lemma: up\n",
      "stem: as -- lemma: a\n",
      "stem: hindu -- lemma: hindu\n",
      "stem: and -- lemma: and\n",
      "stem: i -- lemma: i\n",
      "stem: identifi -- lemma: identify\n",
      "stem: as -- lemma: a\n",
      "stem: hindu -- lemma: hindu\n",
      "stem: thi -- lemma: this\n",
      "stem: is -- lemma: is\n",
      "stem: a -- lemma: a\n",
      "stem: veri -- lemma: very\n",
      "stem: larg -- lemma: large\n",
      "stem: part -- lemma: part\n",
      "stem: of -- lemma: of\n",
      "stem: my -- lemma: my\n",
      "stem: heritag -- lemma: heritage\n",
      "stem: other -- lemma: other\n",
      "stem: way -- lemma: way\n",
      "stem: i -- lemma: i\n",
      "stem: connect -- lemma: connect\n",
      "stem: with -- lemma: with\n",
      "stem: my -- lemma: my\n",
      "stem: cultur -- lemma: culture\n",
      "stem: includ -- lemma: include\n",
      "stem: share -- lemma: sharing\n",
      "stem: food -- lemma: food\n",
      "stem: momo -- lemma: momos\n",
      "stem: are -- lemma: are\n",
      "stem: south -- lemma: south\n",
      "stem: asian -- lemma: asian\n",
      "stem: dumpl -- lemma: dumpling\n",
      "stem: and -- lemma: and\n",
      "stem: theyr -- lemma: theyre\n",
      "stem: my -- lemma: my\n",
      "stem: favorit -- lemma: favorite\n",
      "stem: to -- lemma: to\n",
      "stem: make -- lemma: make\n",
      "stem: and -- lemma: and\n",
      "stem: share -- lemma: share\n",
      "stem: on -- lemma: on\n",
      "stem: my -- lemma: my\n",
      "stem: asian -- lemma: asian\n",
      "stem: american -- lemma: american\n",
      "stem: side -- lemma: side\n",
      "stem: i -- lemma: i\n",
      "stem: am -- lemma: am\n",
      "stem: an -- lemma: an\n",
      "stem: advoc -- lemma: advocate\n",
      "stem: of -- lemma: of\n",
      "stem: immigr -- lemma: immigrant\n",
      "stem: justic -- lemma: justice\n",
      "stem: and -- lemma: and\n",
      "stem: erasur -- lemma: erasure\n",
      "stem: within -- lemma: within\n",
      "stem: apida -- lemma: apida\n",
      "stem: social -- lemma: social\n",
      "stem: or -- lemma: or\n",
      "stem: polit -- lemma: political\n",
      "stem: movement -- lemma: movement\n",
      "stem: i -- lemma: i\n",
      "stem: particip -- lemma: participate\n",
      "stem: in -- lemma: in\n",
      "stem: event -- lemma: event\n",
      "stem: to -- lemma: to\n",
      "stem: embrac -- lemma: embrace\n",
      "stem: my -- lemma: my\n",
      "stem: ident -- lemma: identity\n",
      "stem: such -- lemma: such\n",
      "stem: as -- lemma: a\n",
      "stem: immigr -- lemma: immigrant\n",
      "stem: justic -- lemma: justice\n",
      "stem: advocaci -- lemma: advocacy\n",
      "stem: becaus -- lemma: because\n",
      "stem: i -- lemma: i\n",
      "stem: come -- lemma: come\n",
      "stem: from -- lemma: from\n",
      "stem: a -- lemma: a\n",
      "stem: mixedstatu -- lemma: mixedstatus\n",
      "stem: famili -- lemma: family\n",
      "stem: ive -- lemma: ive\n",
      "stem: alway -- lemma: always\n",
      "stem: been -- lemma: been\n",
      "stem: in -- lemma: in\n",
      "stem: a -- lemma: a\n",
      "stem: commun -- lemma: community\n",
      "stem: with -- lemma: with\n",
      "stem: undocu -- lemma: undocumented\n",
      "stem: asian -- lemma: asian\n",
      "stem: immigr -- lemma: immigrant\n",
      "stem: what -- lemma: what\n",
      "stem: are -- lemma: are\n",
      "stem: some -- lemma: some\n",
      "stem: of -- lemma: of\n",
      "stem: the -- lemma: the\n",
      "stem: challeng -- lemma: challenge\n",
      "stem: you -- lemma: you\n",
      "stem: have -- lemma: have\n",
      "stem: face -- lemma: faced\n",
      "stem: as -- lemma: a\n",
      "stem: an -- lemma: an\n",
      "stem: apida -- lemma: apida\n",
      "stem: individu -- lemma: individual\n",
      "stem: person -- lemma: personally\n",
      "stem: or -- lemma: or\n",
      "stem: profession -- lemma: professionally\n",
      "stem: i -- lemma: i\n",
      "stem: often -- lemma: often\n",
      "stem: struggl -- lemma: struggle\n",
      "stem: with -- lemma: with\n",
      "stem: be -- lemma: being\n",
      "stem: gender -- lemma: gendered\n",
      "stem: as -- lemma: a\n",
      "stem: compliant -- lemma: compliant\n",
      "stem: or -- lemma: or\n",
      "stem: a -- lemma: a\n",
      "stem: pushov -- lemma: pushover\n",
      "stem: profession -- lemma: professionally\n",
      "stem: i -- lemma: i\n",
      "stem: am -- lemma: am\n",
      "stem: often -- lemma: often\n",
      "stem: stereotyp -- lemma: stereotyped\n",
      "stem: as -- lemma: a\n",
      "stem: meek -- lemma: meek\n",
      "stem: so -- lemma: so\n",
      "stem: ive -- lemma: ive\n",
      "stem: been -- lemma: been\n",
      "stem: overlook -- lemma: overlooked\n",
      "stem: for -- lemma: for\n",
      "stem: leadership -- lemma: leadership\n",
      "stem: role -- lemma: role\n",
      "stem: we -- lemma: we\n",
      "stem: are -- lemma: are\n",
      "stem: seen -- lemma: seen\n",
      "stem: as -- lemma: a\n",
      "stem: perpetu -- lemma: perpetually\n",
      "stem: foreign -- lemma: foreign\n",
      "stem: peopl -- lemma: people\n",
      "stem: tend -- lemma: tend\n",
      "stem: to -- lemma: to\n",
      "stem: other -- lemma: other\n",
      "stem: us -- lemma: u\n",
      "stem: in -- lemma: in\n",
      "stem: that -- lemma: that\n",
      "stem: way -- lemma: way\n",
      "stem: yet -- lemma: yet\n",
      "stem: put -- lemma: put\n",
      "stem: us -- lemma: u\n",
      "stem: on -- lemma: on\n",
      "stem: a -- lemma: a\n",
      "stem: pedest -- lemma: pedestal\n",
      "stem: for -- lemma: for\n",
      "stem: what -- lemma: what\n",
      "stem: a -- lemma: a\n",
      "stem: model -- lemma: model\n",
      "stem: minor -- lemma: minority\n",
      "stem: look -- lemma: look\n",
      "stem: like -- lemma: like\n",
      "stem: thi -- lemma: this\n",
      "stem: ha -- lemma: ha\n",
      "stem: made -- lemma: made\n",
      "stem: me -- lemma: me\n",
      "stem: hesit -- lemma: hesitant\n",
      "stem: to -- lemma: to\n",
      "stem: share -- lemma: share\n",
      "stem: my -- lemma: my\n",
      "stem: heritag -- lemma: heritage\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: past -- lemma: past\n",
      "stem: becaus -- lemma: because\n",
      "stem: these -- lemma: these\n",
      "stem: assumpt -- lemma: assumption\n",
      "stem: get -- lemma: get\n",
      "stem: map -- lemma: mapped\n",
      "stem: onto -- lemma: onto\n",
      "stem: me -- lemma: me\n",
      "stem: can -- lemma: can\n",
      "stem: you -- lemma: you\n",
      "stem: describ -- lemma: describe\n",
      "stem: some -- lemma: some\n",
      "stem: common -- lemma: common\n",
      "stem: barrier -- lemma: barrier\n",
      "stem: of -- lemma: of\n",
      "stem: entri -- lemma: entry\n",
      "stem: that -- lemma: that\n",
      "stem: apida -- lemma: apida\n",
      "stem: individu -- lemma: individual\n",
      "stem: specif -- lemma: specifically\n",
      "stem: women -- lemma: woman\n",
      "stem: may -- lemma: may\n",
      "stem: face -- lemma: face\n",
      "stem: when -- lemma: when\n",
      "stem: tri -- lemma: trying\n",
      "stem: to -- lemma: to\n",
      "stem: enter -- lemma: enter\n",
      "stem: or -- lemma: or\n",
      "stem: advanc -- lemma: advance\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: workplac -- lemma: workplace\n",
      "stem: be -- lemma: being\n",
      "stem: overlook -- lemma: overlooked\n",
      "stem: for -- lemma: for\n",
      "stem: leadership -- lemma: leadership\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: past -- lemma: past\n",
      "stem: i -- lemma: i\n",
      "stem: have -- lemma: have\n",
      "stem: not -- lemma: not\n",
      "stem: been -- lemma: been\n",
      "stem: view -- lemma: viewed\n",
      "stem: as -- lemma: a\n",
      "stem: a -- lemma: a\n",
      "stem: leader -- lemma: leader\n",
      "stem: peopl -- lemma: people\n",
      "stem: sometim -- lemma: sometimes\n",
      "stem: have -- lemma: have\n",
      "stem: preconceiv -- lemma: preconceived\n",
      "stem: stereotyp -- lemma: stereotype\n",
      "stem: of -- lemma: of\n",
      "stem: asian -- lemma: asian\n",
      "stem: women -- lemma: woman\n",
      "stem: not -- lemma: not\n",
      "stem: be -- lemma: being\n",
      "stem: abl -- lemma: able\n",
      "stem: to -- lemma: to\n",
      "stem: be -- lemma: be\n",
      "stem: bold -- lemma: bold\n",
      "stem: or -- lemma: or\n",
      "stem: be -- lemma: being\n",
      "stem: vocal -- lemma: vocal\n",
      "stem: can -- lemma: can\n",
      "stem: be -- lemma: be\n",
      "stem: mistaken -- lemma: mistaken\n",
      "stem: for -- lemma: for\n",
      "stem: be -- lemma: being\n",
      "stem: too -- lemma: too\n",
      "stem: emot -- lemma: emotional\n",
      "stem: how -- lemma: how\n",
      "stem: do -- lemma: do\n",
      "stem: you -- lemma: you\n",
      "stem: believ -- lemma: believe\n",
      "stem: microaggress -- lemma: microaggressions\n",
      "stem: impact -- lemma: impact\n",
      "stem: apida -- lemma: apida\n",
      "stem: individu -- lemma: individual\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: workplac -- lemma: workplace\n",
      "stem: can -- lemma: can\n",
      "stem: you -- lemma: you\n",
      "stem: provid -- lemma: provide\n",
      "stem: exampl -- lemma: example\n",
      "stem: of -- lemma: of\n",
      "stem: such -- lemma: such\n",
      "stem: microaggress -- lemma: microaggressions\n",
      "stem: erasur -- lemma: erasure\n",
      "stem: is -- lemma: is\n",
      "stem: big -- lemma: big\n",
      "stem: to -- lemma: to\n",
      "stem: me -- lemma: me\n",
      "stem: onli -- lemma: only\n",
      "stem: say -- lemma: saying\n",
      "stem: merri -- lemma: merry\n",
      "stem: christma -- lemma: christmas\n",
      "stem: isnt -- lemma: isnt\n",
      "stem: inclus -- lemma: inclusive\n",
      "stem: to -- lemma: to\n",
      "stem: other -- lemma: other\n",
      "stem: religion -- lemma: religion\n",
      "stem: peopl -- lemma: people\n",
      "stem: are -- lemma: are\n",
      "stem: often -- lemma: often\n",
      "stem: resist -- lemma: resistant\n",
      "stem: to -- lemma: to\n",
      "stem: say -- lemma: saying\n",
      "stem: happi -- lemma: happy\n",
      "stem: holiday -- lemma: holiday\n",
      "stem: but -- lemma: but\n",
      "stem: say -- lemma: saying\n",
      "stem: merri -- lemma: merry\n",
      "stem: christma -- lemma: christmas\n",
      "stem: exclud -- lemma: excludes\n",
      "stem: and -- lemma: and\n",
      "stem: doe -- lemma: doe\n",
      "stem: not -- lemma: not\n",
      "stem: appreci -- lemma: appreciate\n",
      "stem: my -- lemma: my\n",
      "stem: heritag -- lemma: heritage\n",
      "stem: often -- lemma: often\n",
      "stem: microaggress -- lemma: microaggressions\n",
      "stem: are -- lemma: are\n",
      "stem: not -- lemma: not\n",
      "stem: micro -- lemma: micro\n",
      "stem: at -- lemma: at\n",
      "stem: all -- lemma: all\n",
      "stem: they -- lemma: they\n",
      "stem: typic -- lemma: typically\n",
      "stem: are -- lemma: are\n",
      "stem: not -- lemma: not\n",
      "stem: aggress -- lemma: aggressive\n",
      "stem: racial -- lemma: racialized\n",
      "stem: violenc -- lemma: violence\n",
      "stem: but -- lemma: but\n",
      "stem: the -- lemma: the\n",
      "stem: term -- lemma: term\n",
      "stem: micro -- lemma: micro\n",
      "stem: minim -- lemma: minimizes\n",
      "stem: impact -- lemma: impact\n",
      "stem: some -- lemma: some\n",
      "stem: that -- lemma: that\n",
      "stem: ive -- lemma: ive\n",
      "stem: heard -- lemma: heard\n",
      "stem: are -- lemma: are\n",
      "stem: what -- lemma: what\n",
      "stem: kind -- lemma: kind\n",
      "stem: of -- lemma: of\n",
      "stem: asian -- lemma: asian\n",
      "stem: are -- lemma: are\n",
      "stem: you -- lemma: you\n",
      "stem: or -- lemma: or\n",
      "stem: where -- lemma: where\n",
      "stem: are -- lemma: are\n",
      "stem: you -- lemma: you\n",
      "stem: from -- lemma: from\n",
      "stem: thi -- lemma: this\n",
      "stem: automat -- lemma: automatically\n",
      "stem: make -- lemma: make\n",
      "stem: me -- lemma: me\n",
      "stem: the -- lemma: the\n",
      "stem: other -- lemma: other\n",
      "stem: and -- lemma: and\n",
      "stem: not -- lemma: not\n",
      "stem: seen -- lemma: seen\n",
      "stem: as -- lemma: a\n",
      "stem: american -- lemma: american\n",
      "stem: even -- lemma: even\n",
      "stem: within -- lemma: within\n",
      "stem: the -- lemma: the\n",
      "stem: apida -- lemma: apida\n",
      "stem: commun -- lemma: community\n",
      "stem: south -- lemma: south\n",
      "stem: asian -- lemma: asian\n",
      "stem: are -- lemma: are\n",
      "stem: overlook -- lemma: overlooked\n",
      "stem: as -- lemma: a\n",
      "stem: asian -- lemma: asian\n",
      "stem: how -- lemma: how\n",
      "stem: import -- lemma: important\n",
      "stem: is -- lemma: is\n",
      "stem: represent -- lemma: representation\n",
      "stem: specif -- lemma: specifically\n",
      "stem: apida -- lemma: apida\n",
      "stem: represent -- lemma: representation\n",
      "stem: in -- lemma: in\n",
      "stem: organiz -- lemma: organizational\n",
      "stem: leadership -- lemma: leadership\n",
      "stem: posit -- lemma: position\n",
      "stem: i -- lemma: i\n",
      "stem: want -- lemma: want\n",
      "stem: to -- lemma: to\n",
      "stem: say -- lemma: say\n",
      "stem: that -- lemma: that\n",
      "stem: it -- lemma: it\n",
      "stem: is -- lemma: is\n",
      "stem: import -- lemma: important\n",
      "stem: to -- lemma: to\n",
      "stem: have -- lemma: have\n",
      "stem: someon -- lemma: someone\n",
      "stem: who -- lemma: who\n",
      "stem: look -- lemma: look\n",
      "stem: like -- lemma: like\n",
      "stem: you -- lemma: you\n",
      "stem: in -- lemma: in\n",
      "stem: leadership -- lemma: leadership\n",
      "stem: role -- lemma: role\n",
      "stem: and -- lemma: and\n",
      "stem: it -- lemma: it\n",
      "stem: is -- lemma: is\n",
      "stem: but -- lemma: but\n",
      "stem: those -- lemma: those\n",
      "stem: leader -- lemma: leader\n",
      "stem: may -- lemma: may\n",
      "stem: not -- lemma: not\n",
      "stem: share -- lemma: share\n",
      "stem: the -- lemma: the\n",
      "stem: same -- lemma: same\n",
      "stem: belief -- lemma: belief\n",
      "stem: as -- lemma: a\n",
      "stem: you -- lemma: you\n",
      "stem: certain -- lemma: certain\n",
      "stem: privileg -- lemma: privilege\n",
      "stem: such -- lemma: such\n",
      "stem: as -- lemma: a\n",
      "stem: wealth -- lemma: wealth\n",
      "stem: resourc -- lemma: resource\n",
      "stem: or -- lemma: or\n",
      "stem: lack -- lemma: lack\n",
      "stem: of -- lemma: of\n",
      "stem: interact -- lemma: interaction\n",
      "stem: with -- lemma: with\n",
      "stem: lowersocioeconomicstatu -- lemma: lowersocioeconomicstatus\n",
      "stem: asian -- lemma: asian\n",
      "stem: american -- lemma: american\n",
      "stem: may -- lemma: may\n",
      "stem: caus -- lemma: cause\n",
      "stem: a -- lemma: a\n",
      "stem: differ -- lemma: difference\n",
      "stem: in -- lemma: in\n",
      "stem: commun -- lemma: community\n",
      "stem: polit -- lemma: politics\n",
      "stem: i -- lemma: i\n",
      "stem: do -- lemma: do\n",
      "stem: not -- lemma: not\n",
      "stem: think -- lemma: think\n",
      "stem: the -- lemma: the\n",
      "stem: bamboo -- lemma: bamboo\n",
      "stem: ceil -- lemma: ceiling\n",
      "stem: is -- lemma: is\n",
      "stem: accept -- lemma: acceptable\n",
      "stem: but -- lemma: but\n",
      "stem: the -- lemma: the\n",
      "stem: compani -- lemma: company\n",
      "stem: you -- lemma: you\n",
      "stem: work -- lemma: work\n",
      "stem: for -- lemma: for\n",
      "stem: play -- lemma: play\n",
      "stem: a -- lemma: a\n",
      "stem: big -- lemma: big\n",
      "stem: part -- lemma: part\n",
      "stem: in -- lemma: in\n",
      "stem: your -- lemma: your\n",
      "stem: polit -- lemma: politics\n",
      "stem: and -- lemma: and\n",
      "stem: belief -- lemma: belief\n",
      "stem: align -- lemma: alignment\n",
      "stem: how -- lemma: how\n",
      "stem: do -- lemma: do\n",
      "stem: you -- lemma: you\n",
      "stem: feel -- lemma: feel\n",
      "stem: about -- lemma: about\n",
      "stem: codeswitch -- lemma: codeswitching\n",
      "stem: and -- lemma: and\n",
      "stem: have -- lemma: have\n",
      "stem: you -- lemma: you\n",
      "stem: ever -- lemma: ever\n",
      "stem: felt -- lemma: felt\n",
      "stem: it -- lemma: it\n",
      "stem: necessari -- lemma: necessary\n",
      "stem: to -- lemma: to\n",
      "stem: codeswitch -- lemma: codeswitch\n",
      "stem: i -- lemma: i\n",
      "stem: like -- lemma: like\n",
      "stem: share -- lemma: sharing\n",
      "stem: south -- lemma: south\n",
      "stem: asian -- lemma: asian\n",
      "stem: term -- lemma: term\n",
      "stem: or -- lemma: or\n",
      "stem: connect -- lemma: connecting\n",
      "stem: with -- lemma: with\n",
      "stem: other -- lemma: others\n",
      "stem: that -- lemma: that\n",
      "stem: have -- lemma: have\n",
      "stem: similar -- lemma: similar\n",
      "stem: heritag -- lemma: heritage\n",
      "stem: and -- lemma: and\n",
      "stem: cultur -- lemma: culture\n",
      "stem: a -- lemma: a\n",
      "stem: workplac -- lemma: workplace\n",
      "stem: that -- lemma: that\n",
      "stem: is -- lemma: is\n",
      "stem: welcom -- lemma: welcoming\n",
      "stem: to -- lemma: to\n",
      "stem: go -- lemma: going\n",
      "stem: into -- lemma: into\n",
      "stem: thi -- lemma: this\n",
      "stem: sort -- lemma: sort\n",
      "stem: of -- lemma: of\n",
      "stem: breakout -- lemma: breakout\n",
      "stem: is -- lemma: is\n",
      "stem: refresh -- lemma: refreshing\n",
      "stem: and -- lemma: and\n",
      "stem: make -- lemma: make\n",
      "stem: space -- lemma: space\n",
      "stem: for -- lemma: for\n",
      "stem: us -- lemma: u\n",
      "stem: howev -- lemma: however\n",
      "stem: have -- lemma: having\n",
      "stem: to -- lemma: to\n",
      "stem: codeswitch -- lemma: codeswitch\n",
      "stem: could -- lemma: could\n",
      "stem: also -- lemma: also\n",
      "stem: mean -- lemma: mean\n",
      "stem: a -- lemma: a\n",
      "stem: workplac -- lemma: workplace\n",
      "stem: that -- lemma: that\n",
      "stem: is -- lemma: is\n",
      "stem: not -- lemma: not\n",
      "stem: conduc -- lemma: conducive\n",
      "stem: and -- lemma: and\n",
      "stem: welcom -- lemma: welcoming\n",
      "stem: of -- lemma: of\n",
      "stem: other -- lemma: other\n",
      "stem: cultur -- lemma: culture\n",
      "stem: final -- lemma: finally\n",
      "stem: in -- lemma: in\n",
      "stem: your -- lemma: your\n",
      "stem: opinion -- lemma: opinion\n",
      "stem: what -- lemma: what\n",
      "stem: longterm -- lemma: longterm\n",
      "stem: strategi -- lemma: strategy\n",
      "stem: can -- lemma: can\n",
      "stem: creat -- lemma: create\n",
      "stem: last -- lemma: lasting\n",
      "stem: chang -- lemma: change\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: workplac -- lemma: workplace\n",
      "stem: and -- lemma: and\n",
      "stem: ensur -- lemma: ensure\n",
      "stem: support -- lemma: support\n",
      "stem: equal -- lemma: equality\n",
      "stem: and -- lemma: and\n",
      "stem: inclus -- lemma: inclusion\n",
      "stem: for -- lemma: for\n",
      "stem: apida -- lemma: apida\n",
      "stem: individu -- lemma: individual\n",
      "stem: prior -- lemma: prior\n",
      "stem: to -- lemma: to\n",
      "stem: a -- lemma: a\n",
      "stem: career -- lemma: career\n",
      "stem: in -- lemma: in\n",
      "stem: financi -- lemma: financial\n",
      "stem: aid -- lemma: aid\n",
      "stem: i -- lemma: i\n",
      "stem: did -- lemma: did\n",
      "stem: a -- lemma: a\n",
      "stem: lot -- lemma: lot\n",
      "stem: of -- lemma: of\n",
      "stem: research -- lemma: research\n",
      "stem: relat -- lemma: related\n",
      "stem: to -- lemma: to\n",
      "stem: the -- lemma: the\n",
      "stem: post911 -- lemma: post911\n",
      "stem: immigr -- lemma: immigration\n",
      "stem: of -- lemma: of\n",
      "stem: the -- lemma: the\n",
      "stem: south -- lemma: south\n",
      "stem: asian -- lemma: asian\n",
      "stem: diaspora -- lemma: diaspora\n",
      "stem: thi -- lemma: this\n",
      "stem: background -- lemma: background\n",
      "stem: made -- lemma: made\n",
      "stem: me -- lemma: me\n",
      "stem: heavili -- lemma: heavily\n",
      "stem: reli -- lemma: rely\n",
      "stem: on -- lemma: on\n",
      "stem: grassroot -- lemma: grassroots\n",
      "stem: organ -- lemma: organizing\n",
      "stem: hire -- lemma: hire\n",
      "stem: the -- lemma: the\n",
      "stem: peopl -- lemma: people\n",
      "stem: that -- lemma: that\n",
      "stem: want -- lemma: want\n",
      "stem: to -- lemma: to\n",
      "stem: innov -- lemma: innovate\n",
      "stem: hire -- lemma: hire\n",
      "stem: the -- lemma: the\n",
      "stem: changemak -- lemma: changemakers\n",
      "stem: hire -- lemma: hire\n",
      "stem: the -- lemma: the\n",
      "stem: buttonpush -- lemma: buttonpushers\n",
      "stem: reduc -- lemma: reduce\n",
      "stem: relianc -- lemma: reliance\n",
      "stem: on -- lemma: on\n",
      "stem: white -- lemma: whiteness\n",
      "stem: as -- lemma: a\n",
      "stem: chang -- lemma: change\n",
      "stem: thi -- lemma: this\n",
      "stem: will -- lemma: will\n",
      "stem: becom -- lemma: become\n",
      "stem: natur -- lemma: natural\n",
      "stem: for -- lemma: for\n",
      "stem: the -- lemma: the\n",
      "stem: organ -- lemma: organization\n",
      "stem: and -- lemma: and\n",
      "stem: becom -- lemma: become\n",
      "stem: organiz -- lemma: organizational\n",
      "stem: chang -- lemma: change\n",
      "stem: chang -- lemma: change\n",
      "stem: come -- lemma: come\n",
      "stem: from -- lemma: from\n",
      "stem: us -- lemma: u\n",
      "stem: on -- lemma: on\n",
      "stem: the -- lemma: the\n",
      "stem: ground -- lemma: ground\n",
      "stem: a -- lemma: a\n",
      "stem: huge -- lemma: huge\n",
      "stem: thank -- lemma: thank\n",
      "stem: you -- lemma: you\n",
      "stem: to -- lemma: to\n",
      "stem: arbeena -- lemma: arbeena\n",
      "stem: thapa -- lemma: thapa\n",
      "stem: for -- lemma: for\n",
      "stem: share -- lemma: sharing\n",
      "stem: her -- lemma: her\n",
      "stem: experi -- lemma: experience\n",
      "stem: and -- lemma: and\n",
      "stem: be -- lemma: being\n",
      "stem: vulner -- lemma: vulnerable\n",
      "stem: with -- lemma: with\n",
      "stem: us -- lemma: u\n",
      "stem: your -- lemma: your\n",
      "stem: word -- lemma: word\n",
      "stem: were -- lemma: were\n",
      "stem: inspir -- lemma: inspiring\n",
      "stem: and -- lemma: and\n",
      "stem: the -- lemma: the\n",
      "stem: opportun -- lemma: opportunity\n",
      "stem: to -- lemma: to\n",
      "stem: understand -- lemma: understand\n",
      "stem: your -- lemma: your\n",
      "stem: perspect -- lemma: perspective\n",
      "stem: more -- lemma: more\n",
      "stem: ha -- lemma: ha\n",
      "stem: been -- lemma: been\n",
      "stem: valuabl -- lemma: valuable\n",
      "stem: we -- lemma: we\n",
      "stem: hope -- lemma: hope\n",
      "stem: we -- lemma: we\n",
      "stem: can -- lemma: can\n",
      "stem: becom -- lemma: become\n",
      "stem: better -- lemma: better\n",
      "stem: support -- lemma: support\n",
      "stem: for -- lemma: for\n",
      "stem: the -- lemma: the\n",
      "stem: apida -- lemma: apida\n",
      "stem: commun -- lemma: community\n",
      "stem: as -- lemma: a\n",
      "stem: we -- lemma: we\n",
      "stem: learn -- lemma: learn\n",
      "stem: and -- lemma: and\n",
      "stem: grow -- lemma: grow\n",
      "stem: on -- lemma: on\n",
      "stem: our -- lemma: our\n",
      "stem: journey -- lemma: journey\n",
      "stem: of -- lemma: of\n",
      "stem: cultiv -- lemma: cultivating\n",
      "stem: inclus -- lemma: inclusive\n",
      "stem: growth -- lemma: growth\n"
     ]
    }
   ],
   "source": [
    "# How the words are changing from the lemmatizing\n",
    "for word in article.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbcdc741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may',\n",
       " 'is',\n",
       " 'traditionally',\n",
       " 'known',\n",
       " 'a',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'and',\n",
       " 'pacific',\n",
       " 'islander',\n",
       " 'aapi',\n",
       " 'heritage',\n",
       " 'month',\n",
       " 'this',\n",
       " 'month',\n",
       " 'we',\n",
       " 'celebrate',\n",
       " 'the',\n",
       " 'history',\n",
       " 'and',\n",
       " 'contribution',\n",
       " 'made',\n",
       " 'possible',\n",
       " 'by',\n",
       " 'our',\n",
       " 'aapi',\n",
       " 'friend',\n",
       " 'family',\n",
       " 'and',\n",
       " 'community',\n",
       " 'we',\n",
       " 'also',\n",
       " 'examine',\n",
       " 'our',\n",
       " 'level',\n",
       " 'of',\n",
       " 'support',\n",
       " 'and',\n",
       " 'seek',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'better',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'aapi',\n",
       " 'community',\n",
       " 'in',\n",
       " 'an',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'address',\n",
       " 'real',\n",
       " 'concern',\n",
       " 'and',\n",
       " 'experience',\n",
       " 'we',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'thapa',\n",
       " 'one',\n",
       " 'of',\n",
       " 'codeups',\n",
       " 'financial',\n",
       " 'aid',\n",
       " 'and',\n",
       " 'enrollment',\n",
       " 'manager',\n",
       " 'arbeena',\n",
       " 'identifies',\n",
       " 'a',\n",
       " 'nepali',\n",
       " 'american',\n",
       " 'and',\n",
       " 'desi',\n",
       " 'arbeenas',\n",
       " 'parent',\n",
       " 'immigrated',\n",
       " 'to',\n",
       " 'texas',\n",
       " 'in',\n",
       " '1988',\n",
       " 'for',\n",
       " 'better',\n",
       " 'employment',\n",
       " 'and',\n",
       " 'educational',\n",
       " 'opportunity',\n",
       " 'arbeenas',\n",
       " 'older',\n",
       " 'sister',\n",
       " 'wa',\n",
       " 'five',\n",
       " 'when',\n",
       " 'they',\n",
       " 'made',\n",
       " 'the',\n",
       " 'move',\n",
       " 'to',\n",
       " 'the',\n",
       " 'u',\n",
       " 'arbeena',\n",
       " 'wa',\n",
       " 'born',\n",
       " 'later',\n",
       " 'becoming',\n",
       " 'the',\n",
       " 'first',\n",
       " 'in',\n",
       " 'her',\n",
       " 'family',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'u',\n",
       " 'citizen',\n",
       " 'at',\n",
       " 'codeup',\n",
       " 'we',\n",
       " 'take',\n",
       " 'our',\n",
       " 'effort',\n",
       " 'at',\n",
       " 'inclusivity',\n",
       " 'very',\n",
       " 'seriously',\n",
       " 'after',\n",
       " 'speaking',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'we',\n",
       " 'were',\n",
       " 'taught',\n",
       " 'that',\n",
       " 'the',\n",
       " 'term',\n",
       " 'aapi',\n",
       " 'excludes',\n",
       " 'desiamerican',\n",
       " 'individual',\n",
       " 'hence',\n",
       " 'we',\n",
       " 'will',\n",
       " 'now',\n",
       " 'use',\n",
       " 'the',\n",
       " 'term',\n",
       " 'asian',\n",
       " 'pacific',\n",
       " 'islander',\n",
       " 'desi',\n",
       " 'american',\n",
       " 'apida',\n",
       " 'here',\n",
       " 'is',\n",
       " 'how',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'our',\n",
       " 'conversation',\n",
       " 'with',\n",
       " 'arbeena',\n",
       " 'went',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'celebrate',\n",
       " 'or',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'your',\n",
       " 'heritage',\n",
       " 'and',\n",
       " 'cultural',\n",
       " 'tradition',\n",
       " 'i',\n",
       " 'celebrate',\n",
       " 'nepal',\n",
       " 'version',\n",
       " 'of',\n",
       " 'christmas',\n",
       " 'or',\n",
       " 'dashain',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nineday',\n",
       " 'celebration',\n",
       " 'also',\n",
       " 'known',\n",
       " 'a',\n",
       " 'dussehra',\n",
       " 'i',\n",
       " 'grew',\n",
       " 'up',\n",
       " 'a',\n",
       " 'hindu',\n",
       " 'and',\n",
       " 'i',\n",
       " 'identify',\n",
       " 'a',\n",
       " 'hindu',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'large',\n",
       " 'part',\n",
       " 'of',\n",
       " 'my',\n",
       " 'heritage',\n",
       " 'other',\n",
       " 'way',\n",
       " 'i',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'my',\n",
       " 'culture',\n",
       " 'include',\n",
       " 'sharing',\n",
       " 'food',\n",
       " 'momos',\n",
       " 'are',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'dumpling',\n",
       " 'and',\n",
       " 'theyre',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'to',\n",
       " 'make',\n",
       " 'and',\n",
       " 'share',\n",
       " 'on',\n",
       " 'my',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'side',\n",
       " 'i',\n",
       " 'am',\n",
       " 'an',\n",
       " 'advocate',\n",
       " 'of',\n",
       " 'immigrant',\n",
       " 'justice',\n",
       " 'and',\n",
       " 'erasure',\n",
       " 'within',\n",
       " 'apida',\n",
       " 'social',\n",
       " 'or',\n",
       " 'political',\n",
       " 'movement',\n",
       " 'i',\n",
       " 'participate',\n",
       " 'in',\n",
       " 'event',\n",
       " 'to',\n",
       " 'embrace',\n",
       " 'my',\n",
       " 'identity',\n",
       " 'such',\n",
       " 'a',\n",
       " 'immigrant',\n",
       " 'justice',\n",
       " 'advocacy',\n",
       " 'because',\n",
       " 'i',\n",
       " 'come',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mixedstatus',\n",
       " 'family',\n",
       " 'ive',\n",
       " 'always',\n",
       " 'been',\n",
       " 'in',\n",
       " 'a',\n",
       " 'community',\n",
       " 'with',\n",
       " 'undocumented',\n",
       " 'asian',\n",
       " 'immigrant',\n",
       " 'what',\n",
       " 'are',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'challenge',\n",
       " 'you',\n",
       " 'have',\n",
       " 'faced',\n",
       " 'a',\n",
       " 'an',\n",
       " 'apida',\n",
       " 'individual',\n",
       " 'personally',\n",
       " 'or',\n",
       " 'professionally',\n",
       " 'i',\n",
       " 'often',\n",
       " 'struggle',\n",
       " 'with',\n",
       " 'being',\n",
       " 'gendered',\n",
       " 'a',\n",
       " 'compliant',\n",
       " 'or',\n",
       " 'a',\n",
       " 'pushover',\n",
       " 'professionally',\n",
       " 'i',\n",
       " 'am',\n",
       " 'often',\n",
       " 'stereotyped',\n",
       " 'a',\n",
       " 'meek',\n",
       " 'so',\n",
       " 'ive',\n",
       " 'been',\n",
       " 'overlooked',\n",
       " 'for',\n",
       " 'leadership',\n",
       " 'role',\n",
       " 'we',\n",
       " 'are',\n",
       " 'seen',\n",
       " 'a',\n",
       " 'perpetually',\n",
       " 'foreign',\n",
       " 'people',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'other',\n",
       " 'u',\n",
       " 'in',\n",
       " 'that',\n",
       " 'way',\n",
       " 'yet',\n",
       " 'put',\n",
       " 'u',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pedestal',\n",
       " 'for',\n",
       " 'what',\n",
       " 'a',\n",
       " 'model',\n",
       " 'minority',\n",
       " 'look',\n",
       " 'like',\n",
       " 'this',\n",
       " 'ha',\n",
       " 'made',\n",
       " 'me',\n",
       " 'hesitant',\n",
       " 'to',\n",
       " 'share',\n",
       " 'my',\n",
       " 'heritage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'because',\n",
       " 'these',\n",
       " 'assumption',\n",
       " 'get',\n",
       " 'mapped',\n",
       " 'onto',\n",
       " 'me',\n",
       " 'can',\n",
       " 'you',\n",
       " 'describe',\n",
       " 'some',\n",
       " 'common',\n",
       " 'barrier',\n",
       " 'of',\n",
       " 'entry',\n",
       " 'that',\n",
       " 'apida',\n",
       " 'individual',\n",
       " 'specifically',\n",
       " 'woman',\n",
       " 'may',\n",
       " 'face',\n",
       " 'when',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'or',\n",
       " 'advance',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplace',\n",
       " 'being',\n",
       " 'overlooked',\n",
       " 'for',\n",
       " 'leadership',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'i',\n",
       " 'have',\n",
       " 'not',\n",
       " 'been',\n",
       " 'viewed',\n",
       " 'a',\n",
       " 'a',\n",
       " 'leader',\n",
       " 'people',\n",
       " 'sometimes',\n",
       " 'have',\n",
       " 'preconceived',\n",
       " 'stereotype',\n",
       " 'of',\n",
       " 'asian',\n",
       " 'woman',\n",
       " 'not',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'be',\n",
       " 'bold',\n",
       " 'or',\n",
       " 'being',\n",
       " 'vocal',\n",
       " 'can',\n",
       " 'be',\n",
       " 'mistaken',\n",
       " 'for',\n",
       " 'being',\n",
       " 'too',\n",
       " 'emotional',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'believe',\n",
       " 'microaggressions',\n",
       " 'impact',\n",
       " 'apida',\n",
       " 'individual',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplace',\n",
       " 'can',\n",
       " 'you',\n",
       " 'provide',\n",
       " 'example',\n",
       " 'of',\n",
       " 'such',\n",
       " 'microaggressions',\n",
       " 'erasure',\n",
       " 'is',\n",
       " 'big',\n",
       " 'to',\n",
       " 'me',\n",
       " 'only',\n",
       " 'saying',\n",
       " 'merry',\n",
       " 'christmas',\n",
       " 'isnt',\n",
       " 'inclusive',\n",
       " 'to',\n",
       " 'other',\n",
       " 'religion',\n",
       " 'people',\n",
       " 'are',\n",
       " 'often',\n",
       " 'resistant',\n",
       " 'to',\n",
       " 'saying',\n",
       " 'happy',\n",
       " 'holiday',\n",
       " 'but',\n",
       " 'saying',\n",
       " 'merry',\n",
       " 'christmas',\n",
       " 'excludes',\n",
       " 'and',\n",
       " 'doe',\n",
       " 'not',\n",
       " 'appreciate',\n",
       " 'my',\n",
       " 'heritage',\n",
       " 'often',\n",
       " 'microaggressions',\n",
       " 'are',\n",
       " 'not',\n",
       " 'micro',\n",
       " 'at',\n",
       " 'all',\n",
       " 'they',\n",
       " 'typically',\n",
       " 'are',\n",
       " 'not',\n",
       " 'aggressive',\n",
       " 'racialized',\n",
       " 'violence',\n",
       " 'but',\n",
       " 'the',\n",
       " 'term',\n",
       " 'micro',\n",
       " 'minimizes',\n",
       " 'impact',\n",
       " 'some',\n",
       " 'that',\n",
       " 'ive',\n",
       " 'heard',\n",
       " 'are',\n",
       " 'what',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'asian',\n",
       " 'are',\n",
       " 'you',\n",
       " 'or',\n",
       " 'where',\n",
       " 'are',\n",
       " 'you',\n",
       " 'from',\n",
       " 'this',\n",
       " 'automatically',\n",
       " 'make',\n",
       " 'me',\n",
       " 'the',\n",
       " 'other',\n",
       " 'and',\n",
       " 'not',\n",
       " 'seen',\n",
       " 'a',\n",
       " 'american',\n",
       " 'even',\n",
       " 'within',\n",
       " 'the',\n",
       " 'apida',\n",
       " 'community',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'are',\n",
       " 'overlooked',\n",
       " 'a',\n",
       " 'asian',\n",
       " 'how',\n",
       " 'important',\n",
       " 'is',\n",
       " 'representation',\n",
       " 'specifically',\n",
       " 'apida',\n",
       " 'representation',\n",
       " 'in',\n",
       " 'organizational',\n",
       " 'leadership',\n",
       " 'position',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'important',\n",
       " 'to',\n",
       " 'have',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'you',\n",
       " 'in',\n",
       " 'leadership',\n",
       " 'role',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'but',\n",
       " 'those',\n",
       " 'leader',\n",
       " 'may',\n",
       " 'not',\n",
       " 'share',\n",
       " 'the',\n",
       " 'same',\n",
       " 'belief',\n",
       " 'a',\n",
       " 'you',\n",
       " 'certain',\n",
       " 'privilege',\n",
       " 'such',\n",
       " 'a',\n",
       " 'wealth',\n",
       " 'resource',\n",
       " 'or',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'interaction',\n",
       " 'with',\n",
       " 'lowersocioeconomicstatus',\n",
       " 'asian',\n",
       " 'american',\n",
       " 'may',\n",
       " 'cause',\n",
       " 'a',\n",
       " 'difference',\n",
       " 'in',\n",
       " 'community',\n",
       " 'politics',\n",
       " 'i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'think',\n",
       " 'the',\n",
       " 'bamboo',\n",
       " 'ceiling',\n",
       " 'is',\n",
       " 'acceptable',\n",
       " 'but',\n",
       " 'the',\n",
       " 'company',\n",
       " 'you',\n",
       " 'work',\n",
       " 'for',\n",
       " 'play',\n",
       " 'a',\n",
       " 'big',\n",
       " 'part',\n",
       " 'in',\n",
       " 'your',\n",
       " 'politics',\n",
       " 'and',\n",
       " 'belief',\n",
       " 'alignment',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'about',\n",
       " 'codeswitching',\n",
       " 'and',\n",
       " 'have',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'felt',\n",
       " 'it',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'codeswitch',\n",
       " 'i',\n",
       " 'like',\n",
       " 'sharing',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'term',\n",
       " 'or',\n",
       " 'connecting',\n",
       " 'with',\n",
       " 'others',\n",
       " 'that',\n",
       " 'have',\n",
       " 'similar',\n",
       " 'heritage',\n",
       " 'and',\n",
       " 'culture',\n",
       " 'a',\n",
       " 'workplace',\n",
       " 'that',\n",
       " 'is',\n",
       " 'welcoming',\n",
       " 'to',\n",
       " 'going',\n",
       " 'into',\n",
       " 'this',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'breakout',\n",
       " 'is',\n",
       " 'refreshing',\n",
       " 'and',\n",
       " 'make',\n",
       " 'space',\n",
       " 'for',\n",
       " 'u',\n",
       " 'however',\n",
       " 'having',\n",
       " 'to',\n",
       " 'codeswitch',\n",
       " 'could',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'a',\n",
       " 'workplace',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'conducive',\n",
       " 'and',\n",
       " 'welcoming',\n",
       " 'of',\n",
       " 'other',\n",
       " 'culture',\n",
       " 'finally',\n",
       " 'in',\n",
       " 'your',\n",
       " 'opinion',\n",
       " 'what',\n",
       " 'longterm',\n",
       " 'strategy',\n",
       " 'can',\n",
       " 'create',\n",
       " 'lasting',\n",
       " 'change',\n",
       " 'in',\n",
       " 'the',\n",
       " 'workplace',\n",
       " 'and',\n",
       " 'ensure',\n",
       " 'support',\n",
       " 'equality',\n",
       " 'and',\n",
       " 'inclusion',\n",
       " 'for',\n",
       " 'apida',\n",
       " 'individual',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'a',\n",
       " 'career',\n",
       " 'in',\n",
       " 'financial',\n",
       " 'aid',\n",
       " 'i',\n",
       " 'did',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'research',\n",
       " 'related',\n",
       " 'to',\n",
       " 'the',\n",
       " 'post911',\n",
       " 'immigration',\n",
       " 'of',\n",
       " 'the',\n",
       " 'south',\n",
       " 'asian',\n",
       " 'diaspora',\n",
       " 'this',\n",
       " 'background',\n",
       " 'made',\n",
       " 'me',\n",
       " 'heavily',\n",
       " 'rely',\n",
       " 'on',\n",
       " 'grassroots',\n",
       " 'organizing',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'people',\n",
       " 'that',\n",
       " 'want',\n",
       " 'to',\n",
       " 'innovate',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'changemakers',\n",
       " 'hire',\n",
       " 'the',\n",
       " 'buttonpushers',\n",
       " 'reduce',\n",
       " 'reliance',\n",
       " 'on',\n",
       " 'whiteness',\n",
       " 'a',\n",
       " 'change',\n",
       " 'this',\n",
       " 'will',\n",
       " 'become',\n",
       " 'natural',\n",
       " 'for',\n",
       " 'the',\n",
       " 'organization',\n",
       " 'and',\n",
       " 'become',\n",
       " 'organizational',\n",
       " 'change',\n",
       " 'change',\n",
       " 'come',\n",
       " 'from',\n",
       " 'u',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'to',\n",
       " 'arbeena',\n",
       " 'thapa',\n",
       " 'for',\n",
       " 'sharing',\n",
       " 'her',\n",
       " 'experience',\n",
       " 'and',\n",
       " 'being',\n",
       " 'vulnerable',\n",
       " 'with',\n",
       " 'u',\n",
       " 'your',\n",
       " 'word',\n",
       " 'were',\n",
       " 'inspiring',\n",
       " 'and',\n",
       " 'the',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'your',\n",
       " 'perspective',\n",
       " 'more',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'valuable',\n",
       " 'we',\n",
       " 'hope',\n",
       " 'we',\n",
       " 'can',\n",
       " 'become',\n",
       " 'better',\n",
       " 'support',\n",
       " 'for',\n",
       " 'the',\n",
       " 'apida',\n",
       " 'community',\n",
       " 'a',\n",
       " 'we',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'grow',\n",
       " 'on',\n",
       " 'our',\n",
       " 'journey',\n",
       " 'of',\n",
       " 'cultivating',\n",
       " 'inclusive',\n",
       " 'growth']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatizing the article\n",
    "lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "# joining again for illustrating\n",
    "article_lemmatized = ' '.join(lemmas)\n",
    "\n",
    "# list of the Lemmatized words\n",
    "lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bba445c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a        33\n",
       "the      29\n",
       "and      27\n",
       "to       24\n",
       "in       17\n",
       "of       17\n",
       "i        14\n",
       "you      13\n",
       "is       12\n",
       "asian    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value Count of Words\n",
    "pd.Series(lemmas).value_counts()[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c21fae",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e076176b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "stopword_list[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b938c232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 387 stopwords\n",
      "---\n",
      "may traditionally known asian american pacific islander aapi heritage month month celebrate history contributions made possible aapi friends family community also examine level support seek opportunities better understand aapi community effort address real concerns experiences sat arbeena thapa one codeups financial aid enrollment managers arbeena identifies nepali american desi arbeenas parents immigrated texas 1988 better employment educational opportunities arbeenas older sister five made move us arbeena born later becoming first family us citizen codeup take efforts inclusivity seriously speaking arbeena taught term aapi excludes desiamerican individuals hence use term asian pacific islander desi american apida rest conversation arbeena went celebrate connect heritage cultural traditions celebrate nepals version christmas dashain nineday celebration also known dussehra grew hindu identify hindu large part heritage ways connect culture include sharing food momos south asian dumplings theyre favorite make share asian american side advocate immigrant justice erasure within apida social political movements participate events embrace identity immigrant justice advocacy come mixedstatus family ive always community undocumented asian immigrants challenges faced apida individual personally professionally often struggle gendered compliant pushover professionally often stereotyped meek ive overlooked leadership roles seen perpetually foreign people tend us way yet put us pedestal model minority looks like made hesitant share heritage past assumptions get mapped onto describe common barriers entry apida individuals specifically women may face trying enter advance workplace overlooked leadership past not viewed leader people sometimes preconceived stereotypes asian women not able bold vocal mistaken emotional believe microaggressions impact apida individuals workplace provide examples microaggressions erasure big saying merry christmas isnt inclusive religions people often resistant saying happy holidays saying merry christmas excludes not appreciate heritage often microaggressions not micro typically not aggressive racialized violence term micro minimizes impact ive heard kind asian automatically makes not seen american even within apida community south asians overlooked asian important representation specifically apida representation organizational leadership positions want say important someone looks like leadership roles leaders may not share beliefs certain privileges wealth resources lack interaction lowersocioeconomicstatus asian americans may cause difference community politics not think bamboo ceiling acceptable company work plays big part politics belief alignment feel codeswitching ever felt necessary codeswitch like sharing south asian terms connecting others similar heritage culture workplace welcoming going sort breakout refreshing makes space us however codeswitch could also mean workplace not conducive welcoming cultures finally opinion longterm strategies create lasting change workplace ensure support equality inclusion apida individuals prior career financial aid lot research related post911 immigration south asian diaspora background made heavily rely grassroots organizing hire people want innovate hire changemakers hire buttonpushers reduce reliance whiteness change become natural organization become organizational change change comes us ground huge thank arbeena thapa sharing experiences vulnerable us words inspiring opportunity understand perspective valuable hope become better support apida community learn grow journey cultivating inclusive growth\n"
     ]
    }
   ],
   "source": [
    "words = article.split()\n",
    "filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "print('---')\n",
    "\n",
    "article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "print(article_without_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4eeab3",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "# 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "* Lowercase everything\n",
    "* Normalize unicode characters\n",
    "* Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "939cb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to clean string\n",
    "def basic_clean(string):\n",
    "    string_list = []\n",
    "    # checking to see if string has several strings to change\n",
    "    for i in range(0,len(string)):\n",
    "        \n",
    "        \n",
    "        clean_str = string[i]['content'].lower()\n",
    "        clean_str = unicodedata.normalize('NFKD', clean_str)\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "        clean_str = re.sub(r\"[^a-z0-9'\\s]\", '', clean_str)\n",
    "        # Create a dictionary to store the clean \n",
    "        string_dict = {\n",
    "                        f'string {i}': clean_str\n",
    "                        }\n",
    "        # Appending the dictionary to the list\n",
    "        string_list.append(string_dict)\n",
    "\n",
    "    return string_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40002d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to clean string\n",
    "def basic_clean(string):\n",
    "    string_list = []\n",
    "\n",
    "    clean_str = string.lower()\n",
    "    clean_str = unicodedata.normalize('NFKD', clean_str)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    clean_str = re.sub(r\"[^a-z0-9'\\s]\", '', clean_str)\n",
    "\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1665d195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string_list = []\n",
    "\n",
    "for i in range(0,len(articles)):\n",
    "\n",
    "\n",
    "    string = articles[i]['content'].lower()\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    # Create a dictionary to store the clean \n",
    "    string_dict = {\n",
    "                    f'string {i}': string\n",
    "                    }\n",
    "    # Appending the dictionary to the list\n",
    "    string_list.append(string_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4236cd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'string 0': 'may is traditionally known as asian american and pacific islander aapi heritage month this month we celebrate the history and contributions made possible by our aapi friends family and community we also examine our level of support and seek opportunities to better understand the aapi community\\n\\nin an effort to address real concerns and experiences we sat down with arbeena thapa one of codeups financial aid and enrollment managers\\narbeena identifies as nepali american and desi arbeenas parents immigrated to texas in 1988 for better employment and educational opportunities arbeenas older sister was five when they made the move to the us arbeena was born later becoming the first in her family to be a us citizen\\nat codeup we take our efforts at inclusivity very seriously after speaking with arbeena we were taught that the term aapi excludes desiamerican individuals hence we will now use the term asian pacific islander desi american apida\\nhere is how the rest of our conversation with arbeena went\\nhow do you celebrate or connect with your heritage and cultural traditions\\ni celebrate nepals version of christmas or dashain this is a nineday celebration also known as dussehra i grew up as hindu and i identify as hindu this is a very large part of my heritage \\nother ways i connect with my culture include sharing food momos are south asian dumplings and theyre my favorite to make and share\\non my asian american side i am an advocate of immigrant justice and erasure within apida social or political movements i participate in events to embrace my identity such as immigrant justice advocacy because i come from a mixedstatus family ive always been in a community with undocumented asian immigrants \\nwhat are some of the challenges you have faced as an apida individual personally or professionally\\ni often struggle with being gendered as compliant or a pushover professionally i am often stereotyped as meek so ive been overlooked for leadership roles we are seen as perpetually foreign people tend to other us in that way yet put us on a pedestal for what a model minority looks like this has made me hesitant to share my heritage in the past because these assumptions get mapped onto me \\ncan you describe some common barriers of entry that apida individuals specifically women may face when trying to enter or advance in the workplace\\nbeing overlooked for leadership in the past i have not been viewed as a leader people sometimes have preconceived stereotypes of asian women not being able to be bold or being vocal can be mistaken for being too emotional \\nhow do you believe microaggressions impact apida individuals in the workplace can you provide examples of such microaggressions\\nerasure is big to me only saying merry christmas isnt inclusive to other religions people are often resistant to saying happy holidays but saying merry christmas excludes and does not appreciate my heritage \\noften microaggressions are not micro at all they typically are not aggressive racialized violence but the term micro minimizes impact\\nsome that ive heard are what kind of asian are you or where are you from this automatically makes me the other and not seen as american even within the apida community south asians are overlooked as asian\\nhow important is representation specifically apida representation in organizational leadership positions\\ni want to say that it is important to have someone who looks like you in leadership roles and it is but those leaders may not share the same beliefs as you certain privileges such as wealth resources or lack of interaction with lowersocioeconomicstatus asian americans may cause a difference in community politics i do not think the bamboo ceiling is acceptable but the company you work for plays a big part in your politics and belief alignment\\nhow do you feel about codeswitching and have you ever felt it necessary to codeswitch\\ni like sharing south asian terms or connecting with others that have similar heritage and culture a workplace that is welcoming to going into this sort of breakout is refreshing and makes space for us however having to codeswitch could also mean a workplace that is not conducive and welcoming of other cultures \\nfinally in your opinion what longterm strategies can create lasting change in the workplace and ensure support equality and inclusion for apida individuals\\nprior to a career in financial aid i did a lot of research related to the post911 immigration of the south asian diaspora this background made me heavily rely on grassroots organizing hire the people that want to innovate hire the changemakers hire the buttonpushers reduce reliance on whiteness as change this will become natural for the organization and become organizational change change comes from us on the ground\\na huge thank you to arbeena thapa for sharing her experiences and being vulnerable with us your words were inspiring and the opportunity to understand your perspective more has been valuable we hope we can become better support for the apida community as we learn and grow on our journey of cultivating inclusive growth'},\n",
       " {'string 1': 'women in tech panelist spotlight  magdalena rahn\\ncodeup is hosting a women in tech panel in honor of womens history month on march 29th 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as women in the tech industry\\n\\nmeet magdalena\\nmagdalena rahn is a current codeup student in a data science cohort in san antonio texas she has a professional background in crosscultural communications international business development the wine industry and journalism after serving in the us navy she decided to complement her professional skill set by attending the data science program at codeup she is set to graduate in march 2023 magdalena is fluent in french bulgarian chinesemandarin spanish and italian\\nwe asked magdalena how codeup impacted her career and she replied codeup has provided a solid foundation in analytical processes programming and data science methods and its been an encouragement to have such supportive instructors and wonderful classmates\\ndont forget to tune in on march 29th to sit in on an insightful conversation with magdalena'},\n",
       " {'string 2': 'women in tech panelist spotlight  rachel robbinsmayhill\\ncodeup is hosting a women in tech panel in honor of womens history month on march 29th 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as women in the tech industry meet rachel\\n\\nrachel robbinsmayhill is a decision science analyst i in san antonio texas rachel has had a varied career that includes counseling teaching training community development and military operations her focus has always been on assessing needs identifying solutions and educating individuals and groups on aligning needs and solutions in different contexts rachels passion for data science stems from her belief that data is a powerful tool for communicating patterns that can lead to hope and growth in the future\\nin june 2022 rachel graduated from codeups innis cohort where she honed her skills in data science shortly after she started working as a data science technical writer with apex systems as a contractor for usaa in july 2022 her unconventional role allowed her to understand where her skills could be best utilized to support usaa in a noncontract role\\nrachel recently joined usaas data science delivery team as a decision science analyst i in february 2023 the team is focused on delivering machine learning models for fraud prevention and rachels particular role centers around providing strategic process solutions for the team in collaboration with operational and model risk components\\nin addition to her career rachel is currently pursuing a masters degree in applied data science from syracuse university further expanding her knowledge and skills in the field rachel is passionate about collaborating with individuals who share her belief in the potential of others and strive to achieve growth through logical informed action she welcomes linkedin connections and is excited about supporting the network of codeup alumni\\nwe asked rachel how codeup impacted her career and she replied codeup delivered a comprehensive education in all facets of the data science pipeline laying a strong foundation for me to build upon through repeated handson practice i developed a reliable process that was immediately applicable in my job collaborative group projects were instrumental in helping me hone my skills in project management allowing me to navigate complex data science projects with comfortability thanks to this invaluable experience i was able to make significant strides in my career within just six months of graduating from codeup\\ndont forget to tune in on march 29th to sit in on an insightful conversation'},\n",
       " {'string 3': 'women in tech panelist spotlight  sarah mellor \\ncodeup is hosting a women in tech panel in honor of womens history month on march 29th 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as women in the tech industry\\nmeet sarah\\nsarah mellor currently works as the director of people operations she joined codeup four and a half years ago as an admissions manager she went on to build out and lead the marketing and admissions team while picking up people ops tasks and projects here and there until moving over to lead the people ops team two years ago prior to codeup she worked at educationfocused nonprofits in washington dc and boulder colorado she graduated from wake forest university\\nwe asked sarah how codeup has impacted her career and her response was i have absolutely loved having the privilege to grow alongside codeup in my time here across multiple different roles and departments ive seen a lot of change the consistent things have always been the high quality of passionate and hardworking people i get to work with the impactful mission we get to work on and the inspiring students who trust us with their career change\\ndont forget to tune in on march 29th to sit in on an insightful conversation'},\n",
       " {'string 4': 'women in tech panelist spotlight  madeleine capper\\ncodeup is hosting a women in tech panel in honor of womens history month on march 29th 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as women in the tech industry\\nmeet madeleine\\nmadeleine capper is a data scientist in san antonio texas a longstanding san antonio resident she studied mathematics at the university of texas san antonio and has worked as a data scientist for booz allen hamilton madeleine currently teaches data science at codeup where she works daily with burgeoning data professionals to help them actualize their career aspirations through technical education\\nmadeleine attended codeup as a student in early 2019 as a pupil in the very first codeup data science cohort the program proved immediately effective and she was the first student to obtain a data career out of the program after working at booz allen hamilton madeleines passion for education in conjunction with her appreciation for codeups capacity for transformative life change brought her back to the institution in an instructional capacity where she has been teaching for two years\\ndont forget to tune in on march 29th to sit in on an insightful conversation'},\n",
       " {'string 5': 'black excellence in tech panelist spotlight  wilmarie de la cruz mejia\\n\\ncodeup is hosting a black excellence in tech panel in honor of black history month on february 22 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as black leaders in the tech industry  \\nmeet wilmarie\\nwilmarie de la cruz mejia is a current codeup student on the path to becoming a fullstack web developer at our dallas tx campus \\nwilmarie is a veteran expanding her knowledge of programming languages and technologies on her journey with codeup \\nwe asked wilmarie to share more about her experience at codeup she shares i was able to meet other people who were passionate about coding and be in a positive learning environment\\nwe hope you can join us on february 22nd to sit in on an insightful conversation with wilmarie and all of our panelists'}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0a47540a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'string 0': 'may is traditionally known as asian american and pacific islander aapi heritage month this month we celebrate the history and contributions made possible by our aapi friends family and community we also examine our level of support and seek opportunities to better understand the aapi community\\n\\nin an effort to address real concerns and experiences we sat down with arbeena thapa one of codeups financial aid and enrollment managers\\narbeena identifies as nepali american and desi arbeenas parents immigrated to texas in 1988 for better employment and educational opportunities arbeenas older sister was five when they made the move to the us arbeena was born later becoming the first in her family to be a us citizen\\nat codeup we take our efforts at inclusivity very seriously after speaking with arbeena we were taught that the term aapi excludes desiamerican individuals hence we will now use the term asian pacific islander desi american apida\\nhere is how the rest of our conversation with arbeena went\\nhow do you celebrate or connect with your heritage and cultural traditions\\ni celebrate nepals version of christmas or dashain this is a nineday celebration also known as dussehra i grew up as hindu and i identify as hindu this is a very large part of my heritage \\nother ways i connect with my culture include sharing food momos are south asian dumplings and theyre my favorite to make and share\\non my asian american side i am an advocate of immigrant justice and erasure within apida social or political movements i participate in events to embrace my identity such as immigrant justice advocacy because i come from a mixedstatus family ive always been in a community with undocumented asian immigrants \\nwhat are some of the challenges you have faced as an apida individual personally or professionally\\ni often struggle with being gendered as compliant or a pushover professionally i am often stereotyped as meek so ive been overlooked for leadership roles we are seen as perpetually foreign people tend to other us in that way yet put us on a pedestal for what a model minority looks like this has made me hesitant to share my heritage in the past because these assumptions get mapped onto me \\ncan you describe some common barriers of entry that apida individuals specifically women may face when trying to enter or advance in the workplace\\nbeing overlooked for leadership in the past i have not been viewed as a leader people sometimes have preconceived stereotypes of asian women not being able to be bold or being vocal can be mistaken for being too emotional \\nhow do you believe microaggressions impact apida individuals in the workplace can you provide examples of such microaggressions\\nerasure is big to me only saying merry christmas isnt inclusive to other religions people are often resistant to saying happy holidays but saying merry christmas excludes and does not appreciate my heritage \\noften microaggressions are not micro at all they typically are not aggressive racialized violence but the term micro minimizes impact\\nsome that ive heard are what kind of asian are you or where are you from this automatically makes me the other and not seen as american even within the apida community south asians are overlooked as asian\\nhow important is representation specifically apida representation in organizational leadership positions\\ni want to say that it is important to have someone who looks like you in leadership roles and it is but those leaders may not share the same beliefs as you certain privileges such as wealth resources or lack of interaction with lowersocioeconomicstatus asian americans may cause a difference in community politics i do not think the bamboo ceiling is acceptable but the company you work for plays a big part in your politics and belief alignment\\nhow do you feel about codeswitching and have you ever felt it necessary to codeswitch\\ni like sharing south asian terms or connecting with others that have similar heritage and culture a workplace that is welcoming to going into this sort of breakout is refreshing and makes space for us however having to codeswitch could also mean a workplace that is not conducive and welcoming of other cultures \\nfinally in your opinion what longterm strategies can create lasting change in the workplace and ensure support equality and inclusion for apida individuals\\nprior to a career in financial aid i did a lot of research related to the post911 immigration of the south asian diaspora this background made me heavily rely on grassroots organizing hire the people that want to innovate hire the changemakers hire the buttonpushers reduce reliance on whiteness as change this will become natural for the organization and become organizational change change comes from us on the ground\\na huge thank you to arbeena thapa for sharing her experiences and being vulnerable with us your words were inspiring and the opportunity to understand your perspective more has been valuable we hope we can become better support for the apida community as we learn and grow on our journey of cultivating inclusive growth'},\n",
       " {'string 1': 'women in tech panelist spotlight  magdalena rahn\\ncodeup is hosting a women in tech panel in honor of womens history month on march 29th 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as women in the tech industry\\n\\nmeet magdalena\\nmagdalena rahn is a current codeup student in a data science cohort in san antonio texas she has a professional background in crosscultural communications international business development the wine industry and journalism after serving in the us navy she decided to complement her professional skill set by attending the data science program at codeup she is set to graduate in march 2023 magdalena is fluent in french bulgarian chinesemandarin spanish and italian\\nwe asked magdalena how codeup impacted her career and she replied codeup has provided a solid foundation in analytical processes programming and data science methods and its been an encouragement to have such supportive instructors and wonderful classmates\\ndont forget to tune in on march 29th to sit in on an insightful conversation with magdalena'},\n",
       " {'string 2': 'women in tech panelist spotlight  rachel robbinsmayhill\\ncodeup is hosting a women in tech panel in honor of womens history month on march 29th 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as women in the tech industry meet rachel\\n\\nrachel robbinsmayhill is a decision science analyst i in san antonio texas rachel has had a varied career that includes counseling teaching training community development and military operations her focus has always been on assessing needs identifying solutions and educating individuals and groups on aligning needs and solutions in different contexts rachels passion for data science stems from her belief that data is a powerful tool for communicating patterns that can lead to hope and growth in the future\\nin june 2022 rachel graduated from codeups innis cohort where she honed her skills in data science shortly after she started working as a data science technical writer with apex systems as a contractor for usaa in july 2022 her unconventional role allowed her to understand where her skills could be best utilized to support usaa in a noncontract role\\nrachel recently joined usaas data science delivery team as a decision science analyst i in february 2023 the team is focused on delivering machine learning models for fraud prevention and rachels particular role centers around providing strategic process solutions for the team in collaboration with operational and model risk components\\nin addition to her career rachel is currently pursuing a masters degree in applied data science from syracuse university further expanding her knowledge and skills in the field rachel is passionate about collaborating with individuals who share her belief in the potential of others and strive to achieve growth through logical informed action she welcomes linkedin connections and is excited about supporting the network of codeup alumni\\nwe asked rachel how codeup impacted her career and she replied codeup delivered a comprehensive education in all facets of the data science pipeline laying a strong foundation for me to build upon through repeated handson practice i developed a reliable process that was immediately applicable in my job collaborative group projects were instrumental in helping me hone my skills in project management allowing me to navigate complex data science projects with comfortability thanks to this invaluable experience i was able to make significant strides in my career within just six months of graduating from codeup\\ndont forget to tune in on march 29th to sit in on an insightful conversation'},\n",
       " {'string 3': 'women in tech panelist spotlight  sarah mellor \\ncodeup is hosting a women in tech panel in honor of womens history month on march 29th 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as women in the tech industry\\nmeet sarah\\nsarah mellor currently works as the director of people operations she joined codeup four and a half years ago as an admissions manager she went on to build out and lead the marketing and admissions team while picking up people ops tasks and projects here and there until moving over to lead the people ops team two years ago prior to codeup she worked at educationfocused nonprofits in washington dc and boulder colorado she graduated from wake forest university\\nwe asked sarah how codeup has impacted her career and her response was i have absolutely loved having the privilege to grow alongside codeup in my time here across multiple different roles and departments ive seen a lot of change the consistent things have always been the high quality of passionate and hardworking people i get to work with the impactful mission we get to work on and the inspiring students who trust us with their career change\\ndont forget to tune in on march 29th to sit in on an insightful conversation'},\n",
       " {'string 4': 'women in tech panelist spotlight  madeleine capper\\ncodeup is hosting a women in tech panel in honor of womens history month on march 29th 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as women in the tech industry\\nmeet madeleine\\nmadeleine capper is a data scientist in san antonio texas a longstanding san antonio resident she studied mathematics at the university of texas san antonio and has worked as a data scientist for booz allen hamilton madeleine currently teaches data science at codeup where she works daily with burgeoning data professionals to help them actualize their career aspirations through technical education\\nmadeleine attended codeup as a student in early 2019 as a pupil in the very first codeup data science cohort the program proved immediately effective and she was the first student to obtain a data career out of the program after working at booz allen hamilton madeleines passion for education in conjunction with her appreciation for codeups capacity for transformative life change brought her back to the institution in an instructional capacity where she has been teaching for two years\\ndont forget to tune in on march 29th to sit in on an insightful conversation'},\n",
       " {'string 5': 'black excellence in tech panelist spotlight  wilmarie de la cruz mejia\\n\\ncodeup is hosting a black excellence in tech panel in honor of black history month on february 22 2023 to further celebrate wed like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as black leaders in the tech industry  \\nmeet wilmarie\\nwilmarie de la cruz mejia is a current codeup student on the path to becoming a fullstack web developer at our dallas tx campus \\nwilmarie is a veteran expanding her knowledge of programming languages and technologies on her journey with codeup \\nwe asked wilmarie to share more about her experience at codeup she shares i was able to meet other people who were passionate about coding and be in a positive learning environment\\nwe hope you can join us on february 22nd to sit in on an insightful conversation with wilmarie and all of our panelists'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619ca62",
   "metadata": {},
   "source": [
    "# 2. Define a function named ```tokenize```. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "595a6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # make our tokenizer, taken from nltk's ToktokTokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    # apply our tokenizer's tokenization to the string being input, ensure it returns a string\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73bbcf",
   "metadata": {},
   "source": [
    "# 3. Define a function named ```stem```. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2e4c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # create our stemming object\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    # use a list comprehension => stem each word for each word inside of the entire document,\n",
    "    # split by the default, which are single spaces\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    # glue it back together with spaces, as it was before\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b43fb",
   "metadata": {},
   "source": [
    "# 4. Define a function named ```lemmatize```. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e678b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # create our lemmatizer object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # use a list comprehension to lemmatize each word\n",
    "    # string.split() => output a list of every token inside of the document\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    # glue the lemmas back together by the strings we split on\n",
    "    string = ' '.join(lemmas)\n",
    "    #return the altered document\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dec49",
   "metadata": {},
   "source": [
    "# 5. Define a function named ```remove_stopwords```. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, `extra_words` and `exclude_words`. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92627671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # assign our stopwords from nltk into stopword_list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # utilizing set casting, i will remove any excluded stopwords\n",
    "    stopword_set = set(stopword_list) - set(exclude_words)\n",
    "    # add in any extra words to my stopwords set using a union\n",
    "    stopword_set = stopword_set.union(set(extra_words))\n",
    "    # split our document by spaces\n",
    "    words = string.split()\n",
    "    # every word in our document, as long as that word is not in our stopwords\n",
    "    filtered_words = [word for word in words if word not in stopword_set]\n",
    "    # glue it back together with spaces, as it was so it shall be\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    # return the document back\n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326416bb",
   "metadata": {},
   "source": [
    "# 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daacfd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acquire import get_blog_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d037d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acquire import get_news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4bfb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   What are the schemes & projects approved by Ce...   \n",
       "1            Netherlands slips into recession           \n",
       "2   Workers with traditional skills to get ₹1 lakh...   \n",
       "3   ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "4   What are the schemes & projects approved by Ce...   \n",
       "5            Netherlands slips into recession           \n",
       "6   Workers with traditional skills to get ₹1 lakh...   \n",
       "7   ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "8   What are the schemes & projects approved by Ce...   \n",
       "9            Netherlands slips into recession           \n",
       "10  Workers with traditional skills to get ₹1 lakh...   \n",
       "11  ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "12  What are the schemes & projects approved by Ce...   \n",
       "13           Netherlands slips into recession           \n",
       "14  Workers with traditional skills to get ₹1 lakh...   \n",
       "15  ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "\n",
       "                                              content       category  \n",
       "0   The Union Cabinet on Wednesday approved severa...       business  \n",
       "1   The Dutch economy has entered a recession as i...       business  \n",
       "2   Union Railways Minister Ashwini Vaishnaw on We...       business  \n",
       "3   The Comptroller and Auditor General of India (...       business  \n",
       "4   The Union Cabinet on Wednesday approved severa...         sports  \n",
       "5   The Dutch economy has entered a recession as i...         sports  \n",
       "6   Union Railways Minister Ashwini Vaishnaw on We...         sports  \n",
       "7   The Comptroller and Auditor General of India (...         sports  \n",
       "8   The Union Cabinet on Wednesday approved severa...     technology  \n",
       "9   The Dutch economy has entered a recession as i...     technology  \n",
       "10  Union Railways Minister Ashwini Vaishnaw on We...     technology  \n",
       "11  The Comptroller and Auditor General of India (...     technology  \n",
       "12  The Union Cabinet on Wednesday approved severa...  entertainment  \n",
       "13  The Dutch economy has entered a recession as i...  entertainment  \n",
       "14  Union Railways Minister Ashwini Vaishnaw on We...  entertainment  \n",
       "15  The Comptroller and Auditor General of India (...  entertainment  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = get_news_articles()\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085dd65",
   "metadata": {},
   "source": [
    "# 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95d04ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   What are the schemes & projects approved by Ce...   \n",
       "1            Netherlands slips into recession           \n",
       "2   Workers with traditional skills to get ₹1 lakh...   \n",
       "3   ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "4   What are the schemes & projects approved by Ce...   \n",
       "5            Netherlands slips into recession           \n",
       "6   Workers with traditional skills to get ₹1 lakh...   \n",
       "7   ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "8   What are the schemes & projects approved by Ce...   \n",
       "9            Netherlands slips into recession           \n",
       "10  Workers with traditional skills to get ₹1 lakh...   \n",
       "11  ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "12  What are the schemes & projects approved by Ce...   \n",
       "13           Netherlands slips into recession           \n",
       "14  Workers with traditional skills to get ₹1 lakh...   \n",
       "15  ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "\n",
       "                                              content       category  \n",
       "0   The Union Cabinet on Wednesday approved severa...       business  \n",
       "1   The Dutch economy has entered a recession as i...       business  \n",
       "2   Union Railways Minister Ashwini Vaishnaw on We...       business  \n",
       "3   The Comptroller and Auditor General of India (...       business  \n",
       "4   The Union Cabinet on Wednesday approved severa...         sports  \n",
       "5   The Dutch economy has entered a recession as i...         sports  \n",
       "6   Union Railways Minister Ashwini Vaishnaw on We...         sports  \n",
       "7   The Comptroller and Auditor General of India (...         sports  \n",
       "8   The Union Cabinet on Wednesday approved severa...     technology  \n",
       "9   The Dutch economy has entered a recession as i...     technology  \n",
       "10  Union Railways Minister Ashwini Vaishnaw on We...     technology  \n",
       "11  The Comptroller and Auditor General of India (...     technology  \n",
       "12  The Union Cabinet on Wednesday approved severa...  entertainment  \n",
       "13  The Dutch economy has entered a recession as i...  entertainment  \n",
       "14  Union Railways Minister Ashwini Vaishnaw on We...  entertainment  \n",
       "15  The Comptroller and Auditor General of India (...  entertainment  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = get_news_articles()\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db560d",
   "metadata": {},
   "source": [
    "# 8. For each dataframe, produce the following columns:\n",
    "\n",
    "* ```title``` to hold the title\n",
    "* `original` to hold the original article/post content\n",
    "* `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "* `stemmed` to hold the stemmed version of the cleaned data.\n",
    "* `lemmatized` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da7e2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df.rename(columns={'content':'original'}, inplace=True)\n",
    "# Apply basic_clean, tokenize, and remove_stopwords functions to each row in the 'original' column\n",
    "codeup_df['clean'] = codeup_df['original'].apply(lambda string: remove_stopwords(tokenize(basic_clean(string))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24f4691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stem to each row using using the 'clean' column\n",
    "codeup_df['stemmed'] = codeup_df['clean'].apply(lambda string: stem(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "deed8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatize to each row using using the 'clean' column\n",
    "codeup_df['lemmatized'] = codeup_df['clean'].apply(lambda string: lemmatize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88fb863a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>category</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>sports</td>\n",
       "      <td>comptroller auditor general india cag said 697...</td>\n",
       "      <td>comptrol auditor gener india cag said 697 cror...</td>\n",
       "      <td>comptroller auditor general india cag said 697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>technology</td>\n",
       "      <td>comptroller auditor general india cag said 697...</td>\n",
       "      <td>comptrol auditor gener india cag said 697 cror...</td>\n",
       "      <td>comptroller auditor general india cag said 697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>sports</td>\n",
       "      <td>dutch economy entered recession shrank 03 quar...</td>\n",
       "      <td>dutch economi enter recess shrank 03 quarterli...</td>\n",
       "      <td>dutch economy entered recession shrank 03 quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>technology</td>\n",
       "      <td>dutch economy entered recession shrank 03 quar...</td>\n",
       "      <td>dutch economi enter recess shrank 03 quarterli...</td>\n",
       "      <td>dutch economy entered recession shrank 03 quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>union cabinet wednesday approved several schem...</td>\n",
       "      <td>union cabinet wednesday approv sever scheme pr...</td>\n",
       "      <td>union cabinet wednesday approved several schem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "7   ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "11  ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "5            Netherlands slips into recession           \n",
       "9            Netherlands slips into recession           \n",
       "12  What are the schemes & projects approved by Ce...   \n",
       "\n",
       "                                             original       category  \\\n",
       "7   The Comptroller and Auditor General of India (...         sports   \n",
       "11  The Comptroller and Auditor General of India (...     technology   \n",
       "5   The Dutch economy has entered a recession as i...         sports   \n",
       "9   The Dutch economy has entered a recession as i...     technology   \n",
       "12  The Union Cabinet on Wednesday approved severa...  entertainment   \n",
       "\n",
       "                                                clean  \\\n",
       "7   comptroller auditor general india cag said 697...   \n",
       "11  comptroller auditor general india cag said 697...   \n",
       "5   dutch economy entered recession shrank 03 quar...   \n",
       "9   dutch economy entered recession shrank 03 quar...   \n",
       "12  union cabinet wednesday approved several schem...   \n",
       "\n",
       "                                              stemmed  \\\n",
       "7   comptrol auditor gener india cag said 697 cror...   \n",
       "11  comptrol auditor gener india cag said 697 cror...   \n",
       "5   dutch economi enter recess shrank 03 quarterli...   \n",
       "9   dutch economi enter recess shrank 03 quarterli...   \n",
       "12  union cabinet wednesday approv sever scheme pr...   \n",
       "\n",
       "                                           lemmatized  \n",
       "7   comptroller auditor general india cag said 697...  \n",
       "11  comptroller auditor general india cag said 697...  \n",
       "5   dutch economy entered recession shrank 03 quar...  \n",
       "9   dutch economy entered recession shrank 03 quar...  \n",
       "12  union cabinet wednesday approved several schem...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the a sample of dataframe\n",
    "codeup_df.sample(5).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e395d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.rename(columns={'content':'original'}, inplace=True)\n",
    "# Apply basic_clean, tokenize, and remove_stopwords functions to each row in the 'original' column\n",
    "news_df['clean'] = news_df['original'].apply(lambda string: remove_stopwords(tokenize(basic_clean(string))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62f38284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stem to each row using using the 'clean' column\n",
    "news_df['stemmed'] = news_df['clean'].apply(lambda string: stem(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9979c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatize to each row using using the 'clean' column\n",
    "news_df['lemmatized'] = news_df['clean'].apply(lambda string: lemmatize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36669890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>category</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Workers with traditional skills to get ₹1 lakh...</td>\n",
       "      <td>Union Railways Minister Ashwini Vaishnaw on We...</td>\n",
       "      <td>business</td>\n",
       "      <td>union railways minister ashwini vaishnaw wedne...</td>\n",
       "      <td>union railway minist ashwini vaishnaw wednesda...</td>\n",
       "      <td>union railway minister ashwini vaishnaw wednes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>₹6.9 cr spent for treatment of 3,400 'dead' pa...</td>\n",
       "      <td>The Comptroller and Auditor General of India (...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>comptroller auditor general india cag said 697...</td>\n",
       "      <td>comptrol auditor gener india cag said 697 cror...</td>\n",
       "      <td>comptroller auditor general india cag said 697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the schemes &amp; projects approved by Ce...</td>\n",
       "      <td>The Union Cabinet on Wednesday approved severa...</td>\n",
       "      <td>technology</td>\n",
       "      <td>union cabinet wednesday approved several schem...</td>\n",
       "      <td>union cabinet wednesday approv sever scheme pr...</td>\n",
       "      <td>union cabinet wednesday approved several schem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>sports</td>\n",
       "      <td>dutch economy entered recession shrank 03 quar...</td>\n",
       "      <td>dutch economi enter recess shrank 03 quarterli...</td>\n",
       "      <td>dutch economy entered recession shrank 03 quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Netherlands slips into recession</td>\n",
       "      <td>The Dutch economy has entered a recession as i...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>dutch economy entered recession shrank 03 quar...</td>\n",
       "      <td>dutch economi enter recess shrank 03 quarterli...</td>\n",
       "      <td>dutch economy entered recession shrank 03 quar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "2   Workers with traditional skills to get ₹1 lakh...   \n",
       "15  ₹6.9 cr spent for treatment of 3,400 'dead' pa...   \n",
       "8   What are the schemes & projects approved by Ce...   \n",
       "5            Netherlands slips into recession           \n",
       "13           Netherlands slips into recession           \n",
       "\n",
       "                                             original       category  \\\n",
       "2   Union Railways Minister Ashwini Vaishnaw on We...       business   \n",
       "15  The Comptroller and Auditor General of India (...  entertainment   \n",
       "8   The Union Cabinet on Wednesday approved severa...     technology   \n",
       "5   The Dutch economy has entered a recession as i...         sports   \n",
       "13  The Dutch economy has entered a recession as i...  entertainment   \n",
       "\n",
       "                                                clean  \\\n",
       "2   union railways minister ashwini vaishnaw wedne...   \n",
       "15  comptroller auditor general india cag said 697...   \n",
       "8   union cabinet wednesday approved several schem...   \n",
       "5   dutch economy entered recession shrank 03 quar...   \n",
       "13  dutch economy entered recession shrank 03 quar...   \n",
       "\n",
       "                                              stemmed  \\\n",
       "2   union railway minist ashwini vaishnaw wednesda...   \n",
       "15  comptrol auditor gener india cag said 697 cror...   \n",
       "8   union cabinet wednesday approv sever scheme pr...   \n",
       "5   dutch economi enter recess shrank 03 quarterli...   \n",
       "13  dutch economi enter recess shrank 03 quarterli...   \n",
       "\n",
       "                                           lemmatized  \n",
       "2   union railway minister ashwini vaishnaw wednes...  \n",
       "15  comptroller auditor general india cag said 697...  \n",
       "8   union cabinet wednesday approved several schem...  \n",
       "5   dutch economy entered recession shrank 03 quar...  \n",
       "13  dutch economy entered recession shrank 03 quar...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the a sample of dataframe\n",
    "news_df.sample(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bca216",
   "metadata": {},
   "source": [
    "# 9. Ask yourself:\n",
    "\n",
    "* If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cb84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
